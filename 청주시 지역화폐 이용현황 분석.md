# 청주시 지역화폐 이용현황 분석

- Text: 청주시 지역화폐 결제내역 데이터 및 가입내역 데이터를 자유롭게 분석해 인사이트 도출하기
- 수행 기간: 2021-11-01 ~ 2022-01-21
- 수행인원: 2
- 사용 툴: R, SQL, Tableau
- 배경: 청주 시청에서 인턴십을 하며 수행한 프로젝트입니다.

## 개요

- **목적**
    1. 청주시 지역화폐 이용현황을 분석하여 향후 운영에 활용할 결과를 도출한
- **요구사항**
    1. 청주시 지역화폐 데이터에 대한 EDA 및 자유로운 분석을 통한 인사이트 도출
    
    · 분석항목
    
![Untitled 8](https://github.com/user-attachments/assets/7f7ebf10-a3b3-4d7f-8ee7-22787b400197)
    
- **필요 역량**
    1. Python, R과 같은 분석 툴을 사용해 데이터를 전처리, 분석할 수 있는 역량
    2. BI툴을 이용해 데이터를 시각화 할 수 있는 역량

## 활용 데이터

| **번호** |              **데이터 정보** |             **데이터 명** |                 **기간** |       **출처** |
| --- | --- | --- | --- | --- |
| 1 | 지역화폐 결제 데이터 | 코나 payment | 2019.12.17 ~ 2021.8.31 | ㈜코나아이 |
| 2 | 지역화폐 충전 데이터 | 코나 charge | 2019.12.17 ~ 2021.8.31 | ㈜코나아이 |
| 3 | 지역화폐 업종별 결제 현황 | 코나 업종별 결제현황 | 2019.12.17 ~ 2021.11.10 | ㈜코나아이 |
| 4 | 지역화폐 성연령별 결제 현황 | 코나 성연령별 결제현황 | 2019.12.17 ~ 2021.11.10 | ㈜코나아이 |
| 5 | 지역화폐 날짜별 결제 현황 | 코나 채널별 결제현황 | 2019.12.17 ~ 2021.11.10 | ㈜코나아이 |

## 분석 프로세스

[데이터 전처리]

![Untitled 9](https://github.com/user-attachments/assets/db1aee63-20cb-471f-95a2-da1ec2234f8e)

[현황 분석]

![Untitled 10](https://github.com/user-attachments/assets/59f13786-97d8-444b-932e-6fe89f25ff9a)

[공간 분석]

![Untitled 11](https://github.com/user-attachments/assets/e861a4f6-07ec-4154-b279-78ed41466c6c)

- 청주페이 분석 코드
    
    공공 빅데이터 인터십 과정으로 청주시청에서 지역화폐 데이터를 분석할 때 사용했던 R, SQL 코드를 리뷰합니다.
    
    ```bash
    1) 채널별 결제 데이터 합치기
    #청주페이 웹사이트에서 관리자만 다운 받을 수 있는 채널별 결제현황 데이터들을 하나의 데이터
    프레임으로 결합
    temp=list.files("C:\\Users\\user\\Desktop\\청주페이\\채널별 결제\\데이터")
    cr=data.frame()
    for (i in 1:24){ cr=rbind(cr, read_excel(paste0("C:\\Users\\user\\Desktop\\청주페이\\채널별
    결제\\데이터\\", temp[i]))) }
    write.csv(cr, "C:\\Users\\user\\Desktop\\채널별.csv")
    ```
    
    관리자만 접근할 수 있는 청주페이 관리 포털에서 일자별 결제 내역 데이터를 다운받아 하나로 합친 뒤 태블로를 통해 시각화 시켰습니다. 이후에 결제금액이 높게 나온 일자와 낮게 나온 일자를 검색하여 그 원인을 찾아본 후 보고서에 기록하였습니다. 
    
    ```bash
    2) 업종별 결제 시계열 데이터 합치기
    temp=sort(list.files("C:\\Users\\user\\Desktop\\chung\\업종별 결제현황"))
    for (i in 1:696){ assign(paste0("u",i),(read_excel(paste0("C:\\Users\\user\\Desktop\\chung\\업종별 결제현황\\", temp[i]))))}
    for (i in 1:696){ assign(paste0("u",i), (replace(get(paste0("u",i)), 13, temp[i])))}
    ur=rbind(u1, u2, ..... u693, u694, u695, u696);
    rm(u1); rm(u2); ... rm(u696)
    #업종별 결제 시계열 데이터의 경우 데이터 내에 날짜 열이 존재하지 않아, 파일 이름 변경 프로그램을 이용해 파일 이름을 날짜로 변경해 준 뒤 일자별 결제 데이터를 하나하나 할당해 준 다음 그 데이터 프레임의 13번째 열을 파일의 이름(날짜)으로 만들어 준 뒤 결합 하나의 데이터 프레임으로
    결합
    temp=sort(list.files("C:\\Users\\user\\Desktop\\chung\\업종별 결제현황 코아이"))
    for (i in 1:320){ assign(paste0("k",i),(read_excel(paste0("C:\\Users\\user\\Desktop\\chung\\업종별 결제현황 코아이\\", temp[i]))))}
    for (i in 1:320){ assign(paste0("k",i), (replace(get(paste0("k",i)), 13, temp[i]))) }
    
    for (i in 0:319){ assign(paste0("u", 377+i), rbind(get(paste0("k", 1+i)), get(paste0("u", 377+i)))) }
    #청주페이 홈페이지에서 업종별 결제 시계열 데이터는 각기 다른 카드사 2개로 분류되어 있으므로 또 다른 카드사의 결제 데이터를 다운받은 뒤 위에서 했던 방법으로 날짜 칼럼을 추가해 주고 위의 데이터 프레임과 결합
    ur$`결제 건수 비율`=substr(ur$`결제 건수 비율`, 1, 8)
    ur=rename(ur, c("결제 건수 비율"="DATE")); ur=ur[-14]
    
    write.csv(ur, "C:\\Users\\user\\Desktop\\업종별 결제.csv", row.names=F)
    #위에서 만들어준 날짜 칼럼의 이름과 내용을 날짜에 맞게 변경
    dur=urdur$결제액=dur$`결제 금액`-dur$`결제 취소 금액` dur$DATE=as.Date(dur$DATE)
    dur1=dcast(dur, DATE~`소분류`, value.var="결제액", sum)
    dur2=dcast(dur, DATE~`대분류`, value.var="결제액", sum)
    write.csv(dur1, "C:\\Users\\user\\Desktop\\소분류 업종별 결제.csv", row.names=F)
    dur2$DATE=substr(dur2$DATE, 3, 10)
    dur2$a=rep(20, 696); dur2$a=as.character(dur2$a)
    dur2=unite(dur2, "DATE", a, DATE, sep="")
    dur2$DATE=as.Date(dur2$DATE)
    write_xlsx(dur2, "C:\\Users\\user\\Desktop\\대분류 업종별 결제.xlsx")
    #업종별 시계열 데이터는 대분류, 소분류 칼럼에 업종이름이 값으로 넣어져 있어, 해당 값들을 칼럼 이름으로 날짜별 결제금액을 쉽게 파악할 수 있게 데이터의 형태를 변환
    ```
    
    ```bash
    3) 성, 연령별 채널별
    temp=list.files("C:\\Users\\user\\Desktop\\청주페이\\성연령별\\데이터")
    sr=data.frame()
    for (i in 1:696){ sr=rbind(sr, read_excel(paste0("C:\\Users\\user\\Desktop\\청주페이\\성연령별
    \\데이터\\", temp[i]))) }
    write.csv(sr, "C:\\Users\\user\\Desktop\\성연령별.csv")
    sr$결제액=sr$`결제 금액`-sr$`결제 취소 금액` dsr=dcast(sr, `날짜`~`연령`+`성별`, value.var="결제액", sum)
    write.csv(dsr, "C:\\Users\\user\\Desktop\\성연령별 시계열.csv", row.names=F)
    apply(dsr0[,c(2:15)], 2, sum)
    #성, 연령별로 결제금액을 시계열 데이터로 파악할 수 있게 데이터 변환
    ```
    
    위와 마찬가지로 관리자만 접근할 수 있는 사이트에서 일자별로 다운 받을 수 있는 업종별 결제 금액 데이터를 다운받아 하나로 합쳐 시계열 데이터로 만들어주었습니다. 
    
    ```bash
    4) payment 데이터 결합
    temp = list.files("C:\\Users\\user\\Desktop\\chung\\payment")
    for (i in 2:91){
     assign(paste0("g",i),(read.csv(paste0("C:\\Users\\user\\Desktop\\chung\\payment\\",
    temp[i])))) }
    for (i in 2:91){ assign(paste0("g",i), (replace(get(paste0("g",i)), 12, substr(temp[i], 8,15)))) }
    #파일이름은 데이터의 년,월,주로 표기되어 있고, 데이터 안의 날짜는 년,월로 표기되어 있기 때문에 데이터 안의 날짜를 파일 이름으로 변경
    
    paste0("g",seq(2,91))
    gr=rbind(g2, g3, g4, g5, g6,.....,g90,g91)
    rm(g2);rm(g3);rm(g4);rm(g5);rm(g6);.....rm(g89);rm(g90);rm(g91)
    #seq함수와 Ctrl+F를 통해 g1~g91을 생성하고 이를 rbind, rm(g) 형식으로 변환
    write.csv(gr, "C:\\Users\\user\\Desktop\\payment.csv")
    ```
    
    데이터를 이용하기 위한 기본적인 전처리를 하는 단계입니다. 여러 파일로 분산된 결제 데이터를 하나로 합칩니다.
    
    list.file함수와 for문을 이용해 payment폴더에 있는 파일을 데이터 프레임으로 R로 불러옵니다. 데이터 안에 있는 날짜는 연과 월만 표기되어 있고, 파일의 이름은 주도 표지되어 있기에 날짜 칼럼에 있는 값들을 파일 이름으로 교체해 줍니다. 다음 paste0("g",seq(2,91)) 함수를 이용해 데이터 프레임의 변수명을 출력해 준 다음 ctrl+F로 rbind와 rm을 추가해 줍니다.
    
    ```bash
    5) 환불 데이터 제거
    gr[gr=="NONE "]=NA
    #결제 데이터 내 모든 NONE 문자값을 NA로 변경
    pgr=gr %>% filter(PAYMENT_TYPE=="결제 ")
    fgr=gr %>% filter(PAYMENT_TYPE=="결제 취소 ")
    #결제 payment데이터를 결제와 결제취소로 나눔
    
    fgr=fgr %>% filter(PAYMENT_AMOUNT>0); fgr=fgr[-17]
    pgr=pgr %>% filter(PAYMENT_AMOUNT>0); pgr=pgr[-17]
    #결제액이 0이하인 행을 지우고, 결제와 결제취소가 원소로 있는 칼럼 제거
    romc=row.match(fgr, pgr)
    #함수를 이용해 결제와 결제취소 행이 같은 것을 추출
    write.csv(romc, "C:\\Users\\user\\Desktop\\결제취소매칭.csv")
    #엑셀에서 row.match에서 나타난 인덱스 번호만 추출해 “결제취소매칭 인덱스.txt”로 저장
    pgr=pgr[-as.vector(sort(romc)),]
    inx=read.table("C:\\Users\\user\\Desktop\\결제취소매칭 인덱스.txt")
    fgr=fgr[as.vector(t(inx)),]
    #row.match함수의 결과로 결제취소와 같은 행으로 분류된 행을 결제 데이터에서 제거
    pgr=pgr[,-c(17,18,19)]; fgr=fgr[,-c(17,18,19)]
    #날짜와 불필요한 칼럼은 고려하지 않고 다시 매칭
    fgr$MERCHANT_BIZ_TYPE=substr(fgr$MERCHANT_BIZ_TYPE, 1, 4)
    pgr$MERCHANT_BIZ_TYPE=substr(pgr$MERCHANT_BIZ_TYPE, 1, 4)
    #업종코드에 띄어쓰기가 붙어있는 것과 있지 않은 것이 있어, 같은 자리수로 맞춤
    romc2=row.match(fgr[-12], pgr[-12])#날짜 변수를 제거하고 다시 매칭
    pgr=pgr[-as.vector(sort(romc2)),]
    write.csv(romc2, "C:\\Users\\user\\Desktop\\결제취소매칭2.csv")
    inx2=read.table("C:\\Users\\user\\Desktop\\결제취소매칭 인덱스2.txt")
    fgr=fgr[as.vector(t(inx2)),]
    #위에 했던 결제 취소 데이터와 같은 결제 데이터의 행을 제거
    romc3=row.match(fgr[,-c(1,4,5,6,7,9,12,14)], pgr[,-c(1,4,5,6,7,9,12,14)])
    #고객 정보는 고려하지 않고 결제금액, 트랜잭션 번호, 매장 번호, 카드번호만 남기고 다시 매칭
    pgr=pgr[-as.vector(sort(romc3)),]
    write.csv(romc3, "C:\\Users\\user\\Desktop\\결제취소매칭3.csv")
    inx3=read.table("C:\\Users\\user\\Desktop\\결제취소매칭 인덱스3.txt")
    fgr=fgr[as.vector(t(inx3)),]
    
    pgr[pgr=="UNKNOWN "]=NA
    romc4=row.match(fgr[,-c(1,4,5,6,7,9,12,14,16)], pgr[,-c(1,4,5,6,7,9,12,14,16)])
    #결제금액 까지 고려하지 않고 다시 매칭
    i=c(1:30)
    pgr[romc4[i],]$PAYMENT_AMOUNT=pgr[romc4[i],]$PAYMENT_AMOUNT-fgr[i,]$PAYMENT_AMOU
    NT
    #결제 금액이 다른 경우라 행을 지우지 않고 결제 금액에서 환불 금액을 차감
    write.csv(pgr, "C:\\Users\\user\\Desktop\\환불제거payment.csv")
    ```
    
    영수증 데이터의 경우 환불 처리된 결제 내역도 결제 내역으로 남아 있기 때문에 환불 처리된 결제 내역을 제거해 주어야합니다. 먼저 결제 데이터를 결제와 결제 취소 데이터로 분리해 준 다음에 결제액이 0인 데이터를 제거해 줍니다.
    
    row.match함수를 이용해 결제 데이터와 환불 데이터의 행이 서로 같은 경우를 찾아준 다음, 환불 데이터와 같은 행을 가진 결제 데이터의 행을 제거해 줍니다. 매칭이 전부 되지 않았는데, 물건을 구매한 다음, 다음 주에 환불하면, 환불 데이터와 결제 데이터의 날짜가 일치하지 않게 됩니다. 날짜 데이터를 제거하고 위 과정을 반복합니다.
    
    여전히 매칭되지 않은 행이 있는데, 고객에 데이터가 결측치였다가 새로 생기는 경우도 있고, 주소나 아이디가 바뀌는 경우가 있었습니다. 카드번호와 결제한 물건, 구입 매장 데이터와 트랜잭션 번호로만 row.match해줍니다.
    
    여러 물건을 사고 일부 물건만 환불한 경우 결제 데이터와 환불 데이터와 가격이 다르게 표기됩니다. 이 경우 row.match한 다음 행을 제거하는 것이 아닌 물건 가격을 빼주는 것으로 대체합니다.
    
    ```bash
    6)나잇대 칼럼 추가
    pgr$PAYMENT_AMOUNT=as.integer(pgr$PAYMENT_AMOUNT)
    pgr$USER_ID=as.character(pgr$USER_ID)
    pgr$BC_MERCHANT_CODE=as.character(pgr$BC_MERCHANT_CODE)
    pgr$USER_BIRTH_YEAR=substr(pgr$USER_BIRTH_YEAR, 3,4)
    #데이터의 속성이 잘못된 것을 바꾸고, 출생년도의 뒤 2자리 수만 남김#나잇대
    pgr$age=0
    pgr$age=ifelse(92 <pgr$USER_BIRTH_YEAR & pgr$USER_BIRTH_YEAR <100 |
    pgr$USER_BIRTH_YEAR==1 | pgr$USER_BIRTH_YEAR==0 | pgr$USER_BIRTH_YEAR==2 |
    pgr$USER_BIRTH_YEAR==93 |pgr$USER_BIRTH_YEAR==94 |pgr$USER_BIRTH_YEAR==95
    |pgr$USER_BIRTH_YEAR==96 |pgr$USER_BIRTH_YEAR==97 | pgr$USER_BIRTH_YEAR==98 |
    pgr$USER_BIRTH_YEAR==99, "20대", pgr$age)
    pgr$age=ifelse( 82< pgr$USER_BIRTH_YEAR & pgr$USER_BIRTH_YEAR <93, '30대', pgr$age)
    pgr$age=ifelse( 72< pgr$USER_BIRTH_YEAR & pgr$USER_BIRTH_YEAR <83, '40대', pgr$age)
    pgr$age=ifelse( 62< pgr$USER_BIRTH_YEAR & pgr$USER_BIRTH_YEAR <73, '50대', pgr$age)
    pgr$age=ifelse( 52< pgr$USER_BIRTH_YEAR & pgr$USER_BIRTH_YEAR <63, '60대', pgr$age)
    pgr$age=ifelse(20< pgr$USER_BIRTH_YEAR & pgr$USER_BIRTH_YEAR<53 , '70대', pgr$age)
    pgr$age=ifelse(pgr$USER_BIRTH_YEAR==2 | pgr$USER_BIRTH_YEAR==3 |
    pgr$USER_BIRTH_YEAR==4 | pgr$USER_BIRTH_YEAR==5 |pgr$USER_BIRTH_YEAR==6
    |pgr$USER_BIRTH_YEAR==7 |pgr$USER_BIRTH_YEAR==8 |pgr$USER_BIRTH_YEAR==9 , "10대",
    pgr$age )
    #청주페이 사용자의 출생연도를 이용해 나잇대를 알게 해주는 칼럼 추가
    ```
    
    현재 데이터로 세세한 나이에 따른 구분은 불필요하다고 생각되어, 나잇대에 따라 구분하기 위해 나잇대 칼럼을 만들어주었습니다.
    
    ```bash
    7)군집분석
    pgr2=pgr[,c(4,5,10,12,16)]
    pgr2=pgr2 %>% filter(GENDER != "UNKNOWN ")
    dpgr2=dcast(pgr2, GENDER+USER_BIRTH_YEAR+MERCHANT_BIZ_TYPE+PAYMENT_APPROVAL_DATE~"PAYMENTAMOUNT", value.var = "PAYMENT_AMOUNT", sum)
    dpgr2=na.omit(dpgr2)
    
    #성별, 출생연도, 업종, 결제일 별 결제 금액의 합이 나타나게 데이터 변환
    adpgr=dcast(dpgr2[,-4], GENDER+USER_BIRTH_YEAR~MERCHANT_BIZ_TYPE, value.var = "PAYMENT_AMOUNT", sum)
    adpgr=na.omit(adpgr)
    
    colnames(adpgr)=substr(colnames(adpgr), 1,2)
    adpgr=rename(adpgr,c("GE"="GENDER", "US"="USER_BIRTH_YEAR"))
    #4개의 값으로 되어있는 업종코드의 앞 2개는 업종을 대분류하는 값#대분류 값으로 dcast함수를 이용해 같은 이름의 열끼리 합칠려고 하였지만 오류가 나기 때문에 for문과 apply함수를 이용해 직접 합침업종 결제액을 직접 계산for (i in c(3:22,24:34)){ assign(paste0("a", i), apply(adpgr[,colnames(adpgr)==unique(colnames(adpgr))[i]], 1, sum)) }
    a23=adpgr$`62`
    apply(adpgr[,colnames(adpgr)==99], 1, sum)
    paste0("a", seq(3, 34))
    agr=cbind(adpgr$GENDER, adpgr$BALANCE_CHARGE_APPROVAL_AMOUNT, a3, ... a34)
    colnames(agr)=c("GENDER", "USER_BIRTH_YEAR", "10","11","20","21","22","30","31","32","33","34","40","41","42","43","44","50","51","52","60","61","62","70","71","80","83","84","90","91","92","93","96","99")
    agr=as.data.frame(agr)
    for (i in (3:34)){ agr[,i]=as.numeric(agr[,i])}
    
    #군집분석을 위한 정규화
    mean=apply(agr[,-c(1,2,35)], 2, mean)
    std=apply(agr[,-c(1,2,35)], 2, sd)
    agr2=agr
    agr2=scale(agr[-c(1,2,35)], center=mean, scale=std)
    agr2=cbind(agr[,c(1,2,35)], agr2)
    library(NbClust)
    nb= NbClust(agr2[,-c(1,2,3)], min.nc = 5, max.nc = 30, method = "kmeans")
    #적절 군집 수를 파악
    library(cluster)
    cl=pam(agr2[,-c(1,2)], k=11, stand=F, diss=F, "euclidean")
    #군집을 구분하는 칼럼을 추가
    write.csv(agr, "C:\\Users\\user\\Desktop\\sedae.csv", row.names = F)
    write.csv(cbind(agr, cl$clustering), "C:\\Users\\user\\Desktop\\c11.csv", row.names = F)
    ```
    
    위에서 날짜에 따른 업종별 결제 금액을 파악하였지만, 이번엔 성, 연령별로 혹은 사람의 성향별로 업종별 결제 금액이 다른지 궁금하였고, 이를 확인하기 위해 군집분석을 시행하였습니다. 우선 성, 연령별로 각 업종에 사용한 결제 금액을 나타내기 위해 데이터를 변환하였습니다. 데이터의 열 이름으로 되어있는 4자리의 숫자는 업종을 소분류 하는 코드이며, 앞의 2자리는 업종을 대분류 하는 코드입니다. 소분류별 결제 금액과 대분류별 결제금액 모두 파악하도록 합니다. 그 뒤 apply함수와 colnames==unique(colnames)를 통해 앞의 이름 2글자가 같은 열끼리 더해 줄 수 있었습니다. 앞의 2자리 수가 같은 칼럼끼리 더해주어 대분류별 결제 금액 역시 파악하였습니다.
    
    큰 값을 가진 칼럼의 영향력이 비대해 지는 것을 막고자 정규화한 뒤 군집분석을 수행하였습니다. 그런 다음write.csv(cbind(agr, 군집분석 결과)를 해주면 군집 번호와 나잇대가 합쳐져 데이터가 산출됩니다. 군집분석 결과 군집 간의 주로 결제하는 업종이 의미 있게 차이가 난다면 나잇대 칼럼처럼 군집 칼럼을 새로 추가해 분석에 이용할 계획이었
    
    지만, 10대가 학원에 주로 소비하고, 노인이 병원에 주로 소비한다는 뻔한 결과를 제외한다면, 군집 간 큰 차이가 나지 않았기에 이 분석 결과는 이후에 사용하지 않았습니다.
    
    ```bash
    8)시계열로 된 데이터 프레임 만들기
    spgr=pgr
    #주소 변환
    spgr=separate(spgr, USER_ADDRESS, c("ad1", "ad2", "ad3", "ad4"), sep = "\\ ")
    spgr$MERCHANT_BIZ_TYPE=substr(spgr$MERCHANT_BIZ_TYPE, 1,2)
    spgr=spgr[,c(4,5,8,10,13,15,19)]
    spgr$ad3=ifelse(spgr$ad3=="남이면", "서원구", spgr$ad3);spgr$ad3=ifelse(spgr$ad3=="가덕면", "상당구", spgr$ad3);spgr$ad3=ifelse(spgr$ad3=="강내면", "흥덕구",spgr$ad3);spgr$ad3=ifelse(spgr$ad3=="강외면", "흥덕구", spgr$ad3)
    spgr$ad3=ifelse(spgr$ad3 == "흥덕구" | spgr$ad3 == "서원구" | spgr$ad3 == "청원구" | spgr$ad3 == "상당구", spgr$ad3, NA)
    #고객의 주소 칼럼의 원소를 4등분 하여 “구”가 포함된 칼럼을 남기고, “구”로 표현되지 않은 값을 “구”로 변경#날짜로 변환
    spgr$PAYMENT_APPROVAL_DATE=as.character(spgr$PAYMENT_APPROVAL_DATE)spg r$ PAYMEN T_ AP PROVAL _ DATE= as .Dat e ( spg r$ P AYMENT_ A PP ROVAL_ DA TE, format='%Y%m%d')
    #시계열로 변환
    dspgr1=dcast(spgr, PAYMENT_APPROVAL_DATE~GENDER,)
    #남녀별 이용자 수를 날짜에 따라 볼 수 있게 변환
    dspgr2=dcast(spgr, PAYMENT_APPROVAL_DATE~ad3,)
    #청주시 구별 이용자 수를 날짜에 따라 볼 수 있게 변환
    dspgr3=dcast(spgr, PAYMENT_APPROVAL_DATE~MERCHANT_BIZ_TYPE,)
    #업종별 이용자 수를 날짜에 따라 볼 수 있게 변환
    dspgr4=dcast(spgr, PAYMENT_APPROVAL_DATE~sedae,)
    #나잇대별 이용자 수를 날짜에 따라 볼 수 있게 변환
    dspgr5=dcast(spgr, PAYMENT_APPROVAL_DATE~USER_GRADE_TYPE,)
    #회원등급별 이용자 수를 날짜에 따라 볼 수 있게 변환#dspgr6=dcast(spgr, PAYMENT_APPROVAL_DATE~“PAYMENT_AMOUNT”, value.var ="PAYMENT_AMOUNT", sum)#날짜별로 청주페이 사용액을 볼 수 있게 변환하려 했지만, 오류가 나므로 for문을 이용해 직접 계산for (i in c(1:90)){
     assign(paste0("a",i), sum((spgr %>% filter(PAYMENT_APPROVAL_DATE==unique(spgr$PAYM
    ENT_APPROVAL_DATE)[i]))$PAYMENT_AMOUNT)) }
    paste0("a",seq(1,90))
    PAYMENT_AMOUNT=rbind(a1,a2.....,a89,a90)
    dspgr6=cbind(dspgr5[1], PAYMENT_AMOUNT)
    #해당 방법으로 날짜별 사용액을 볼 수 있게 변환
    dspgr=cbind(dspgr1, dspgr2, dspgr3, dspgr4, dspgr5, dspgr6)
    dspgr=dspgr[,-c(5,11,45,54,58)]
    write.csv(dspgr, "C:\\Users\\user\\Desktop\\청페시계열.csv", row.names = F)
    #위에 변환한 데이터들을 하나로 합쳐서 생성, payment데이터는 일별이 아닌 주별로 나타나기에 날짜의 간격이 7일 간격
    write.csv(dspgr, "C:\\Users\\user\\Desktop\\dspgr.csv", row.names = F)
    ```
    
    날짜순으로 각 구에서, 성별로, 나잇대 별로, 업종별로 결제 추세를 파악하는 것 역시 관심사입니다. 결제 회원 수와 결 금액은 높은 상관성을 보여 결제한 회원 수를 원소값으로 해당 데이터를 만들어 줍니다. 저희는 고객의 주소의 “구”만 보고
    
    싶어서 주소의 원소값을 띄어쓰기 기준으로 4등분 하여 3번째에 있는 “구” 주소만을 사용하였습니다. 그 다음 위에서 했던 대로 출생연도로 나잇대 칼럼을 만들어 주고, dcast와 for문을 이용해 행을 날짜로 열을 구, 나잇대, 성별로 하여 시간이 지날수록 고객의 이용수가 어떻게 변하는지 파악합니다. 해당 분석 결과 구나 회원등급 별로 유의미한 차이가 나타나지 않았고, 해당 분석의 결과는 무의미 했기에 레포트에 싣지 않았습니다.
    
    ```bash
    9) charge 데이터 결합
    temp = list.files("C:\\Users\\user\\Desktop\\chung\\charge")
    hr=data.frame()
    for (i in 1:90){ hr=rbind(hr, (read.csv(paste0("C:\\Users\\user\\Desktop\\chung\\charge\
    \", temp[i])))) }
    write.csv(hr, "C:\\Users\\user\\Desktop\\charge.csv")
    hr=hr[,-c(2,11,13,15,17)]
    hr=hr %>% filter(BALANCE_CHARGE_APPROVAL_AMOUNT>0 | POLICY_CASH_CHARGE_APPR
    OVAL_AMOUNT>0)
    write.csv(hr, "C:\\Users\\user\\Desktop\\charge01.csv")
    #충전내역 데이터를 결합한 뒤 불필요한 칼럼을 제거한 뒤 충전액이 0 이하인 행을 제거
    ```
    
    청주페이는 충전량의 10%가 되는 청주페이를 인센티브로 지급하고 그 인센티브는 과거엔 최대 3만원 까지 지급했다가, 이후에 5만원으로 증가하고, 7만원 까지 증가하는 이벤트를 한 적이 있습니다. 인센티브 한도의 증가에 따른 충전량의 변화 역시 알아봐야 하는 것 중 하나입니다. 먼저 주별로 분리된 충전 데이터를 하나로 합칩니다.
    
    ```bash
    10) charge 데이터 범주화
    dhr=hr
    dhr$CHARGE_APPROVAL_DATE=substr(dhr$CHARGE_APPROVAL_DATE, 1,6)
    dhr$결제액대=0
    dhr$결제액대=ifelse( 0< dhr$BALANCE_CHARGE_APPROVAL_AMOUNT & dhr$BALANCE_CHARGE_APPROVAL_AMOUNT <299999, '30만미만', dhr$결제액대)
    dhr$결제액대=ifelse( dhr$BALANCE_CHARGE_APPROVAL_AMOUNT==300000, '30만', dhr$결제액대)
    dhr$결제액대=ifelse( 300001< dhr$BALANCE_CHARGE_APPROVAL_AMOUNT & dhr$BALANCE_CHARGE_APPROVAL_AMOUNT <499999, '30초과,50미만', dhr$결제액대)
    dhr$결제액대=ifelse( dhr$BALANCE_CHARGE_APPROVAL_AMOUNT==500000, '50만', dhr$결제액대)
    
    dhr$결제액대=ifelse( 500001< dhr$BALANCE_CHARGE_APPROVAL_AMOUNT & dhr$BALANCE_CHARGE_APPROVAL_AMOUNT <699999, '50초과,70미만', dhr$결제액대)
    dhr$결제액대=ifelse( dhr$BALANCE_CHARGE_APPROVAL_AMOUNT==700000, '70만', dhr$결제액대)
    dhr$결제액대=ifelse( 700001< dhr$BALANCE_CHARGE_APPROVAL_AMOUNT & dhr$BALANCE_CHARGE_APPROVAL_AMOUNT <999999, '70초과,100미만', dhr$결제액대)
    dhr$결제액대=ifelse( dhr$BALANCE_CHARGE_APPROVAL_AMOUNT==1000000, '100만', dhr$결제액대)
    dhr$결제액대=ifelse( 1000001< dhr$BALANCE_CHARGE_APPROVAL_AMOUNT & dhr$BALANCE_CHARGE_APPROVAL_AMOUNT <1999999, '100초과,200미만', dhr$결제액대)
    dhr$결제액대=ifelse( dhr$BALANCE_CHARGE_APPROVAL_AMOUNT==2000000, '200만', dhr$결제액대)
    #충전금액을 범주화
    dhr1=dcast(dhr, CHARGE_APPROVAL_DATE~`결제액대`)
    #충전금액 범주별 충전 회원 수를 시계열 데이터로 볼 수 있게 변환
    write.csv(dhr1, "C:\\Users\\user\\Desktop\\인센티브 효과 보기.csv"
    
    a=hr %>% filter(POLICY_CASH_CHARGE_APPROVAL_AMOUNT > 0)
    a=dcast(a, USER_ID+USER_JOIN_DATE+GENDER+USER_BIRTH_YEAR+CHARGE_APPROVAL_D
    ATE~CASH_NAME, value.var = "POLICY_CASH_CHARGE_APPROVAL_AMOUNT", sum)
    #충전금액이 0원 이상(지원금을 받은 것이 아닌)인 충전내역 데이터를 시계열 데이터로 변환
    write.csv(a, "C:\\Users\\user\\Desktop\\정책지원금 충전량.csv", row.names=F)
    ```
    
    충전금액을 30만원 미만, 30만원, 30만~50만, 50만원, 50만~70만, 70만원, 70만~100만, 100만, 100만~200만으로 분류하는 칼럼을 만든 뒤, 위에서 했던 것처럼 dcast함수로 시계열 데이터로 만든 뒤 시간에 따른 충전금액 변화를 알아봤습니다. 인센티브 상승이 있을 경우 충전금액 증가가 눈에 띄게 증가하는 것을 알 수 있었습니다.
    
    ```bash
    #이탈고객 분석
    library(RSQLite); library(DBI) con = dbConnect(SQLite(), dbname = "C:\....\\Chinook.db")
    funnel=dcast(pgr, USER_ID+CARD_NO+GENDER+USER_BIRTH_YEAR+USER_ADDRESS+age ~substr(PAYMENT_APPROVAL_DATE, 1, 6), value.var="PAYMENT_AMOUNT", sum )
    
    #유저아이디, 카드번호, 성별, 나이, 주소, 그리고 월별 결제금액이 나타나도록 데이터를 전처리
    funnel1=funnel[,7:27]
    funnel1[funnel1>0]=1
    #위와 동일한 데이터 프레임에 결제금액이 1원 이상인 값을 1로 변경하여 결제 금액이 아닌 결제 여부로 구분할 수 있게 함
    funnel2=data.frame()
    for (i in 1:nrow(funnel1)){
    result=c(funnel1[i,1])
     for (j in 1:ncol(funnel1)){
     if (isTRUE(funnel1[i,j] != funnel1[i,j+1])) {result=append(result, funnel1[i,j+1])} }
    result=as.data.frame(t(result))
    result=unite(result, "col", sep="")
    #결제 여부가 0,1로 나타난 다수의 칼럼을 하나의 칼럼으로 합친 뒤 같은 숫자가 연속되어 나오면 한번만 나오게 함 (예: 1111000111101 → 10101)
    
    funnel2=rbind(funnel2, result)
    funnel2=cbind(funnel[,c(1:6)], funnel2)
    col2=unite(funnel1, "col2", sep="")
    col=funnel2$col
    funnel=cbind(funnel, col, col2)
    #결제 여부가 0,1로 나타난 다수의 칼럼을 하나의 칼럼으로 합친 뒤 해당 칼럼 역시 데이터 프레임에 추가
    dbWriteTable(con,"funn", funnel)
    #해당 데이터 프레임을 SQL 테이블로 저장
    ```
    
    유저아이디, 주소, 카드번호를 기반으로 회원을 개인별로 분류합니다. 이 방법은 회원이 회원정보를 변경하면 같은 회원일지라도 다른 회원으로 나타난다는 단점이 있지만, 이를 보완하고 개개인을 정확하게 식별할 수 있는 방법이 존재하지 않았
    
    기에 이를 이용하여 개인을 식별합니다. 그 뒤 월별 결제금액을 나타나게 하고 결제를 하지 않은 달과 그렇지 않은 달을 구분할 수 있게 데이터를 변환해 줍니다.
    
    (1110111101111) → (10101)처럼 연속적으로 중복된 숫자가 나오는 것은 제일 처음 나온 수가 다음에 나오는 수와 다르면 칼럼에 in하고 같으면 out시키는 방식으로 처리하였습니다. 이 전처리로 인하여 col=01을 하면 중간에 유입되어 청주페이를 매달 꾸준히 사용한 고객을 쉽게 찾을 수 있게 됩니다.
    
    ```bash
    12) 고객 분류
    #gogag=dbGetQuery(con, " select * from funnel where col like '%1' or (col2 like '%101%' and col2 like '%10') ")######데이터의 가장 최근 달에 결제 내역이 있거나, 최근 1달에 결제하지 않았어도 1달 동안 이탈했다가 복귀한 이력이 있을 시 충성도가 높은 고객으로 분류
    gogag=dbGetQuery(con, " select * from funn where col like '%1' or (col2 like '%1111110' and col in ('010', '10')) or (col2 like '%11111100' and col in ('010', '10')) or (col2 like '%111100'and col2 like '%1001%' and col in ('01010', '1010')) or (col2 like '%111100' and col2 like'%101%' and col in ('01010', '1010')) or (col2 like '%11110' and col2 like '%101%' and col in('01010', '1010')) or (col2 like '%11110' and col2 like '%1001%' and col in ('01010', '1010')) ")
    dbWriteTable(con, 'gogag', gogag)
    #2019년 12월~2021년 8월 데이터 가장 최근인 2021년 8월에 결제 내역이 있거나, 유입된 후 이탈 없이 6개월 이상 꾸준히 사용했다가 최근 이탈한 지 2달 이내인 경우, 한번의 이탈이 있었지만 그 기간이 2개월을 넘지 않고 최근 이탈 1, 2달 이전의 4개월 동안 청주페이를 연속적으로 사용하였을 경우를 고객으로 분류
    etal=dbGetQuery(con, " select * from funnel where col2 not in (select distinct col2 from gogag) ")
    #위의 기준을 만족하지 않은 나머지 고객을 이탈고객으로 분류
    plot(apply(gogag[7:27], 2, sum), type='l')
    plot(apply(etal[7:27], 2, sum), type='l')
    #충성고객과 이탈고객의 결제 총합을 그래프로 확인#충성고객의 결제 총합은 점점 증가, 이탈고객의 경우 1차 재난지원금 때 급상승했다가 하락
    gogag$pred=c(0); etal$pred=c(1)
    funnel=rbind(gogag, etal)
    dbWriteTable(con,"funnel", funnel)
    #충성고객은 0으로, 이탈고객은 1로 분류하는 칼럼을 만들어 준 뒤 다시 SQL에 테이블로 저장
    ```
    
    기업마다 이탈고객의 기준이 정해져 있었지만 청주페이의 경우 정해진 것이 없었고, 제 임의로 설정할 수밖에 없었습니다. 페이스북 같은 경우 고객의 접속이 매무 빈번하여 2주만 지나도 이탈고객으로 분류한다고 합니다. 청주페이의 경우 편의점이나 음식점같이 생활 밀착형 소비에 많이 쓰기에 최근 1달만 사용하지 않아도 이탈고객으로 분류하려 했습니다. 하지만 주무관님께선 꾸준히 사용하였는데 최근 1달 사용하지 않았다고 해서 이탈고객으로 분류하는 것은 이상하다 하여 6개월 이상 연속적으로 사용했는데 최근 사용하지 않은 고객도 이탈하지 않은 고객으로 분류하였습니다.
    
    ```bash
    13) 데이터 변환(2)
    funnel=dbGetQuery(con, " select * from funnel a left join pgr b on a.USER_ID=b.USER_ID
    and a.CARD_NO=b.cARD_NO and a.GENDeR=b.GENDER and a.USER_BIRTH_YEAR=b.USER_BI
    RTH_YEAR and a.USER_ADDRESS=b.USER_ADDRESS ")
    jfunnel=jfunnel[,-c(31,32,33,34,35,36,48)]
    djfunnel = dcast(jfunnel, USER_ID+CARD_NO+GENDER+USER_BIRTH_YEAR+USER_ADDRESS+age+`201912`+`202001`+`202002`+`202003`+`202004`+`202005`+`202006`+`202007`+`202008`+`202009`+`202010`+`202011`+`202012`+`202101`+`202102`+ `202103`+`202104`+`202105`+`202106`+`202107`+`202108`+col+ col2+pred+MERCHANT_NAME+MERCHANT_SALES_CODE+MERCHANT_BIZ_TYPE+BC_MERCHANT_CODE+MERCHANT_ADDRESS1~"PAYMENT_AMOUNT", value.var = "PAYMENT_AMOUNT", sum)
    dbWriteTable(con,"djfunnel", djfunnel)
    #위에서 생성했던 데이터 프레임은 고객의 결제패턴을 알기 위한 분류였으므로, 상점에 결제되는 패턴을 알기위한 데이터 프레임 역시 생성
    ```
    
    결제패턴이 같은 고객이 어느 업소에서 많이 결제하였는지 알아보기 위해, 고객 데이터에 업소 데이터를 추가로 붙여줍니다.
    
    ```bash
    13) 결제패턴 확인
    dbGetQuery(con, " select substr(MERCHANT_BIZ_TYPE, 1, 2) ,sum(PAYMENT_AMOUNT) from djfunnel where pred=0 group by substr(MERCHANT_BIZ_TYPE, 1, 2) order by sum(PAYMENT_AMOUNT) desc ")
    #일반고객의 대분류 업종별 결제금액
    dbGetQuery(con, " select substr(MERCHANT_BIZ_TYPE, 1, 2) ,sum(PAYMENT_AMOUNT) from djfunnel where pred=1 group by substr(MERCHANT_BIZ_TYPE, 1, 2) order by sum(PAYMENT_AMOUNT) desc ")
    #이탈고객의 대분류 업종별 결제금액
    dbGetQuery(con, " select MERCHANT_BIZ_TYPE, sum(PAYMENT_AMOUNT) from djfunnel where pred=0 group by MERCHANT_BIZ_TYPE order by sum(PAYMENT_AMOUNT) desc ")
    #일반고객의 소분류 업종별 결제금액
    dbGetQuery(con, " select MERCHANT_BIZ_TYPE, sum(PAYMENT_AMOUNT) from djfunnel where pred=1 group by MERCHANT_BIZ_TYPE order by sum(PAYMENT_AMOUNT) desc ")
    #이탈고객의 소분류 업종별 결제금액
    dbGetQuery(con, " select MERCHANT_BIZ_TYPE, sum(PAYMENT_AMOUNT) from djfunnel where pred=1 group by MERCHANT_BIZ_TYPE order by sum(PAYMENT_AMOUNT) desc ")
    #이탈고객이 각 업종에서 사용한 결제액들
    dbGetQuery(con, " select count(pred) from funnel group by pred ")
    #군집 별 수
    dbGetQuery(con, " select col2, count(*) from funnel where pred=1 group by col2 order by count(*)desc limit 50 ")
    #이탈고객의 결제패턴
    dbGetQuery(con, " select col2, count(*) from funnel where pred=0 group by col2 order by count(*) desc limit 50 ")#일반고객 결제패턴
    View(dbGetQuery(con, " select * from funnel where col2='000011000000000000000' "))
    #이해할 수 없는 패턴이 나올 때마다 해당 코드로 고객 정보를 확인한 뒤 카드 내역을 확인해 어느 곳에서 결제를 했는지를 확인#4월 인센티브 증가 때 들어온 사람 분석
    dbGetQuery(con, " select * from funnel where col2 like '00001%' ")
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '00001%' group by pred ")
    #고객과 이탈고객 회원수
    dbGetQuery(con, " select col2, count(*) from funnel where col2 like '00001%' group by col2 order by count(*) desc limit 20 ")
    #4월에 들어온 사람의 결제패턴
    ```
    
    전처리와 고객의 분류가 끝낸 뒤 특정 결제패턴을 가진 고객들을 추출하는 방식으로 전반적인 분석을 끝냈습니다. 분석 결과 유입이 많은 달은 청주에서만 사용할 수 있는 지원금이 나오는 경우이며, 이로 인해 유입된 경우 지원금만 사용하고 이탈하는 경우도 많다는 것을 확인할 수 있었습니다. 반면의 인센티브를 증가시켜 유입이 많아진 경우 지원금 보다 유입되는
    
    인원수는 많지 않지만 이탈하는 사람의 수도 많지 않다는 것을 확인할 수 있었습니다.
    
    ```bash
    #4월에 들어온 사람의 결제패턴
    a=dbGetQuery(con, " select col2 from funnel where col2 like '00001%' group by col2 order
    by count(*) desc limit 20 ")
    b=data.frame()
    for (i in 1:20){ b=rbind(b, apply( (funnel %>% filter(col2==a[i,]))[7:27], 2, mean)) }
    write_xlsx(b, "C:\\Users\\user\\Desktop\\b.xlsx")
    #4월 들어온 사람들의 결제패턴별 결제금액 그래프를 태블로를 이용해 그래프로 그리기 위한
    write.csv
    dbGetQuery(con, " select substr(USER_ADDRESS, 1, 17), count(*) from funnel where col2 like
    '00001%' and pred=0 group by substr(USER_ADDRESS, 1, 17) order by count(*) desc ")#동별
    4월 유입된 충성고객의 수
    dbGetQuery(con, " select GENDER, count(*) from funnel where col2 like '00001%' and pred=0
    group by GENDER order by count(*) desc ")
    #4월 들어온 충성고객의 성별별 수
    dbGetQuery(con, " select age, count(*) from funnel where col2 like '00001%' and pred=0
    group by age order by count(*) desc ")
    #4월 들어온 충성고객의 나잇대별 수
    dbGetQuery(con, " select substr(USER_ADDRESS, 1, 17), count(*) from funnel where col2 like
    '00001%' and pred=1 group by substr(USER_ADDRESS, 1, 17) order by count(*) desc ")#동별
    4월 유입된 이탈고객의 수
    dbGetQuery(con, " select GENDER, count(*) from funnel where col2 like '00001%' and pred=1
    group by GENDER order by count(*) desc ")
    #4월 들어온 이탈고객의 성별별 수
    dbGetQuery(con, " select age, count(*) from funnel where col2 like '00001%' and pred=1 group by age order by count(*) desc ")
    #4월 들어온 이탈고객의 나잇대별 수#1차 재난 지원금때 들어온 사람 분석
    dbGetQuery(con, " select col2, count(*) from funnel where col2 like '000001%' or col2 like '0000001%' group by col2 order by count(*) desc limit 20 ")
    #5,6월에 들어온 사람 결제패턴
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '000001%' or col2 like'0000001%' group by pred ")
    #5,6월에 들어온 고객과 이탈고객 회원 수
    a=dbGetQuery(con, " select col2 from funnel where col2 like '000001%' or col2 like '0000001%' group by col2 order by count(*) desc limit 20 ")
    b=data.frame()
    for (i in 1:20){ b=rbind(b, apply( (funnel %>% filter(col2==a[i,]))[7:27], 2, mean)) }
    write_xlsx(b, "C:\\Users\\user\\Desktop\\b.xlsx")
    #4월 들어온 사람들의 결제패턴별 결제금액 그래프를 태블로를 이용해 그래프로 그리기 위한 write.csv
    dbGetQuery(con, " select substr(USER_ADDRESS, 1, 17), count(*) from funnel where (col2 like '000001%' or col2 like '0000001%') and pred=0 group by substr(USER_ADDRESS, 1, 17) order by count(*) desc ")
    #동별 5,6월 들어온 충성고객의 수
    dbGetQuery(con, " select GENDER, count(*) from funnel where (col2 like '000001%' or col2 like '0000001%') and pred=0 group by GENDER order by count(*) desc ")
    #5,6월 들어온 충성고객의 성별별 수
    dbGetQuery(con, " select age, count(*) from funnel where (col2 like '000001%' or col2 like '0000001%') and pred=0 group by age order by count(*) desc ")
    #5,6월 들어온 충성고객의 나잇대별 수
    dbGetQuery(con, " select substr(USER_ADDRESS, 1, 17), count(*) from funnel where (col2 like '000001%' or col2 like '0000001%') and pred=1 group by substr(USER_ADDRESS, 1, 17) order by count(*) desc ")
    #동별 5,6월 들어온 이탈고객의 수
    dbGetQuery(con, " select GENDER, count(*) from funnel where (col2 like '000001%' or col2 like '0000001%') and pred=1 group by GENDER order by count(*) desc ")
    #5,6월 들어온 이탈고객의 성별별 수
    dbGetQuery(con, " select age, count(*) from funnel where (col2 like '000001%' or col2 like '0000001%') and pred=1 group by age order by count(*) desc ")
    #5,6월 들어온 이탈고객의 나잇대별 수#12월 인센티브 증가 이벤트 들어온 고객 분석
    dbGetQuery(con, " select col2, count(*) from funnel where col2 like '0000000000001%' group by col2 order by count(*) desc limit 20 ")
    #12월에 들어온 고객의 결제 패턴
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '0000000000001%' group by pred ")
    #12월에 들어온 고객과 이탈고객 회원수
    a=dbGetQuery(con, " select col2 from funnel where col2 like '0000000000001%' group by col2 order by count(*) desc limit 20 ")
    b=data.frame()
    for (i in 1:20){ b=rbind(b, apply( (funnel %>% filter(col2==a[i,]))[7:27], 2, mean)) }
    write_xlsx(b, "C:\\Users\\user\\Desktop\\b.xlsx")
    #12월 들어온 사람들의 결제패턴별 결제금액 그래프를 태블로를 이용해 그래프로 그리기 위한
    write.csv
    dbGetQuery(con, " select substr(USER_ADDRESS, 1, 17), count(*) from funnel where col2 like'0000000000001%' and pred=0 group by substr(USER_ADDRESS, 1, 17) order by count(*) desc ")
    #동별 12월 들어온 충성고객의 수
    dbGetQuery(con, " select GENDER, count(*) from funnel where col2 like '0000000000001%' and pred=0 group by GENDER order by count(*) desc ")
    #12월 들어온 충성고객의 성별별 수
    dbGetQuery(con, " select age, count(*) from funnel where col2 like '0000000000001%' and pred=0 group by age order by count(*) desc ")
    #12월 들어온 충성고객의 나잇대별 수
    dbGetQuery(con, " select substr(USER_ADDRESS, 1, 17), count(*) from funnel where col2 like '0000000000001%' and pred=1 group by substr(USER_ADDRESS, 1, 17) order by count(*) desc ")
    #동별 12월 들어온 이탈고객의 수
    dbGetQuery(con, " select GENDER, count(*) from funnel where col2 like'0000000000001%' and pred=1 group by GENDER order by count(*) desc ")
    #12월 들어온 이탈고객의 성별별 수
    dbGetQuery(con, " select age, count(*) from funnel where col2 like '0000000000001%' and pred=1 group by age order by count(*) desc ")
    #12월 들어온 이탈고객의 나잇대별 수#2차 학생 식재료 지원을 받는 것을 계기로 들어온 고객 분석
    a=hr %>% filter(CASH_NAME=='제 2차 학생 식재료 지원')
    #충전 내역 전체 데이터에서 2차 학생 식재료 지원을 받은 고객만 필터링
    ch2sik=dcast(a, USER_ID+USER_JOIN_DATE+CARD_NO+GENDER+USER_BIRTH_YEAR+USER_ADDRESS+CHARGE_APPROVAL_DATE~"CHARGE_AMOUNT", value.var = "CHARGE_AMOUNT", sum)
    dbWriteTable(con, "ch2sik", ch2sik)
    #2차 학생 식재료 지원을 받은 충전내역 데이터에서 회원정보가 같은 내역을 하나로 합쳐 개인별로분류한 뒤 SQL에 테이블로 저장
    a=dbGetQuery(con, " select * from funnel a inner join ch2sik b on a.CARD_NO=b.CARD_NO ");
    
    a=a[,-c(33:36)]
    dbWriteTable(con, "a", a)
    #위에서 만들었던 월별 결제금액과 패턴이 나와있는 데이터 프레임에 있던 고객 중 2차 학생 식재료 지원을 받은 고객만 추출한 뒤 SQL에 테이블로 저장
    dbGetQuery(con, " select col2, count(*) from funnel a inner join ch2sik b on a.CARD_NO=b.CARD_NO group by col2 order by count(*) desc limit 20 ")
    #2차 학생 식재료 지원을 받은 고객의 결제 패턴 확인
    dbGetQuery(con, " select pred, count(*) from funnel a inner join ch2sik b on a.CARD_NO=b.CARD_NO group by pred ")
    #2차학생 식재료 지원을 받은 고객 중 충성고객과 이탈고객이 얼마나 되는지 확인
    a=dbGetQuery(con, " select col2 from funnel where col2 like '0000000000001%' group by col2 order by count(*) desc limit 20 ")
    b=data.frame()
    for (i in 1:20){ b=rbind(b, apply( (funnel %>% filter(col2==a[i,]))[7:27], 2, mean)) }
    write_xlsx(b, "C:\\Users\\user\\Desktop\\b.xlsx")
    #2차 식재료 지원금을 받은 사람들의 결제패턴별 결제금액 그래프를 태블로를 이용해 그래프로 그리기 위한 write.csv
    dbGetQuery(con, " select substr(USER_ADDRESS, 1, 17), count(*) from a where pred=0 group by substr(USER_ADDRESS, 1, 17) order by count(*) desc ")
    #2차 학생 식재료 지원을 받은 충성고객의 동별 회원 수 확인
    dbGetQuery(con, " select substr(USER_ADDRESS, 1, 17), count(*) from a where pred=1 group by substr(USER_ADDRESS, 1, 17) order by count(*) desc ")
    #2차 학생 식재료 지원을 받은 이탈고객의 동별 회원 수 확인
    dbGetQuery(con, " select GENDER, count(*) from a where pred=0 group by GENDER order by count(*) desc ")
    #2차 학생 식재료 지원을 받은 충성고객의 성별별 회원 수 확인
    dbGetQuery(con, " select GENDER, count(*) from a where pred=1 group by GENDER order by count(*) desc ")
    #2차 학생 식재료 지원을 받은 이탈고객의 성별별 회원 수 확인
    dbGetQuery(con, " select age, count(*) from a where pred=0 group by age order by count(*) desc ")
    #2차 학생 식재료 지원을 받은 충성고객의 나잇대별 회원 수 확인
    dbGetQuery(con, " select age, count(*) from a where pred=1 group by age order by count(*) desc ")
    #2차 학생 식재료 지원을 받은 이탈고객의 나잇대별 회원 수 확인
    dbGetQuery(con, 'drop table a'); dbGetQuery(con, 'drop table ch2sik')
    #월별 충성고객과 이탈고객
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '1%' group by pred ")
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '01%' group by pred ")
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '001%' group by pred ")
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '0001%' group by pred ")
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '00001%' group by pred
    ")
    ....
    ....
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '00000000000000001%' group by pred ")
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '000000000000000001%' group by pred ")
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '0000000000000000001%' group by pred ")
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '00000000000000000001%' group by pred ")
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '000000000000000000001%' group by pred ")
    dbGetQuery(con, " select pred, count(*) from funnel where col2 like '0000000000000000000001%' group by pred ")
    #각 월에 유입된 회원 중 충성고객과 이탈고객이 된 회원 수 확인
    ```
    
    ```bash
    #상권 추세 분석
    14) 데이터 변환
    msc=dcast(pgr, BC_MERCHANT_CODE+MERCHANT_BIZ_TYPE~PAYMENT_APPROVAL_DATE, value.var = "PAYMENT_AMOUNT", sum)
    msc1=dcast(pgr, MERCHANT_BIZ_TYPE+BC_MERCHANT_CODE~MERCHANT_SALES_CODE)
    #가맹점의 매출구간 등급이 나타나게 데이터를 변환
    msc2=msc1
    msc2$`8 `=ifelse(msc1$`8 ` > 0, 1, 0)
    msc2$`A `=ifelse(msc1$`A ` > 0, 1, 0)
    msc2$`J `=ifelse(msc1$`J ` > 0, 1, 0)
    msc2$`K `=ifelse(msc1$`K ` > 0, 1, 0)
    msc2$`S `=ifelse(msc1$`S ` > 0, 1, 0)
    msc2$`Z `=ifelse(msc1$`Z ` > 0, 1, 0)
    #매출구간 등급의 값이 1 이상이면 1이 되게 변환
    msinx=which(apply(msc2[,c(3:8)], 1, sum) > 1)
    #which함수로 매출 구간의 변화가 일어난 적 있는 가맹점의 인덱스 추출
    msc3=msc1[msinx,]
    msc3=msc3[-9]
    #매출구간 변화가 있던 가맹점만 추출, 44364개의 가맹점 중 사업장의 규모가 바뀐 적 있는 가게는 2815개
    mgr=pgr[,c(8:16)]
    mgr$PAYMENT_APPROVAL_DATE=substr(mgr$PAYMENT_APPROVAL_DATE, 1, 6)
    mgr=dcast(mgr, MERCHANT_NAME+MERCHANT_SALES_CODE+MERCHANT_BIZ_TYPE+BC_MERCHANT_CODE+PAYMENT_APPROVAL_DATE+MERCHANT_ADDRESS1+MERCHANT_ZIP_CODE~"PAYSUM", value.var = "PAYMENT_AMOUNT", sum )
    tmgr=data.frame()
    for (i in 1:2040){ tmgr=rbind(tmgr, mgr %>% filter(mgr$MERCHANT_BIZ_TYPE==msc3$MERCHANT_BIZ_TYPE[i] & mgr$BC_MERCHANT_CODE==msc3$BC_MERCHANT_CODE[i])) }
    
    #가맹점의 월별 매출구간 등급을 쉽게 볼 수 있게 변환
    tmgr1=data.frame()
    for (i in 1:2039){ tmgr1=rbind(tmgr1 ,(tmgr %>% filter(tmgr$BC_MERCHANT_CODE==unique(tmgr$BC_MERCHANT_CODE)[i]))[c(1,nrow( (tmgr %>% filter(tmgr$BC_MERCHANT_CODE==unique(tmgr$BC_MERCHANT_CODE)[i])))),]) }
    #위에 만든 데이터의 업소의 첫 번째 달 행과 마지막 달 행만 추출하여 데이터 프레임을 만듦
    tmgr1$MERCHANT_SALES_CODE[tmgr1$MERCHANT_SALES_CODE=="8 "]=3
    tmgr1$MERCHANT_SALES_CODE[tmgr1$MERCHANT_SALES_CODE=="A "]=5
    tmgr1$MERCHANT_SALES_CODE[tmgr1$MERCHANT_SALES_CODE=="J "]=10
    tmgr1$MERCHANT_SALES_CODE[tmgr1$MERCHANT_SALES_CODE=="K "]=30
    tmgr1$MERCHANT_SALES_CODE[tmgr1$MERCHANT_SALES_CODE=="S "]=3
    tmgr1$MERCHANT_SALES_CODE[tmgr1$MERCHANT_SALES_CODE=="Z "]=100
    tmgr1$MERCHANT_SALES_CODE=as.integer(tmgr1$MERCHANT_SALES_CODE)
    #문자열로 된 매출규모 등급을 숫자로 변경
    tmgr1$idx=c(1:4076)
    tmgr1[tmgr1$idx%%2==1,]$MERCHANT_SALES_CODE=tmgr1[tmgr1$idx%%2==1,]$MERCHANT_SALES_CODE*-1
    tmgr1=tmgr1[-9]
    tmgr2=dcast(tmgr1, MERCHANT_BIZ_TYPE+BC_MERCHANT_CODE~"scale change", value.var = "MERCHANT_SALES_CODE", sum )
    tmgr2=tmgr2 %>% filter(`scale change`!=0)
    
    #마지막달 매출규모 등급에서 첫 번쨰 달 매출규모 등급을 뺀 값을 이용해 매출규모 등급 변화를 알아냄
    write.csv(tmgr2, "C:\\Users\\user\\Desktop\\tmgr.csv", row.names = F)
    up=tmgr2 %>% filter(tmgr2$`scale change` > 0)
    down=tmgr2 %>% filter(tmgr2$`scale change` < 0)
    #청주페이 가맹점 중 규모가 커진곳은 950곳, 작아진 곳은 1088곳이 있다. a=dcast(tmgr2[,c(1,3)], MERCHANT_BIZ_TYPE~`scale change`)
    write.csv(a, "C:\\Users\\user\\Desktop\\tmgrVer2.csv", row.names = F)
    #지역별 증감
    tmgr3=dcast(tmgr1, MERCHANT_BIZ_TYPE+BC_MERCHANT_CODE+MERCHANT_ADDRESS1~“scale change", value.var = "MERCHANT_SALES_CODE", sum )
    tmgr3=tmgr3 %>% filter(`scale change`!=0)
    
    #위에 만든 데이터에 주소 칼럼을 추가
    mykey =""
    library("ggmap")
    library("readxl")
    register_google(key=mykey)
    geocode(location = base::enc2utf8("청주시"), source = 'google')
    tmgr3$MERCHANT_ADDRESS1 <- enc2utf8(tmgr3$MERCHANT_ADDRESS1)
    eout=mutate_geocode(data = tmgr3, location = MERCHANT_ADDRESS1, source = 'google')
    up=eout %>% filter(eout$`scale change` > 0)
    down=eout %>% filter(eout$`scale change` < 0)
    #지오코딩으로 주소를 이용해 x,y좌표를 따옴 → 이후 QGIS로 분석
    write.csv(up, "C:\\Users\\user\\Desktop\\지역별증감U.csv", row.names=F)
    write.csv(down, "C:\\Users\\user\\Desktop\\지역별증감D.csv", row.names=F)
    #매출 규모등급이 아닌 매출 추세를 이용한 증감 파악
    mae=pgr
    mae$PAYMENT_APPROVAL_DATE=substr(mae$PAYMENT_APPROVAL_DATE, 1, 6)
    mae=dcast(mae, PAYMENT_APPROVAL_DATE~BC_MERCHANT_CODE, value.var = "PAYMENT_AMOUNT", sum)
    #칼럼은 매장 고유번호를 행은 날짜, 원소값은 해당 매장의 해당 날짜 매출이 되게 데이터를 변환
    #plot이미지 저장
    setwd("G:\\plot")
    for ( i in 2:15689){
     png(paste0(i,".png"))
     plot(lowess(mae[i]), type="l")
     dev.off()
    }
    #매장들의 결제액 시계열 그래프를 확인하기 위해 저장한 후 탐색
    ```
    
    매장별로 매출액의 증감, 그리고 감소가 많은 지역과 증가가 많은 지역을 분류하는 것 역시 한번 해봐야 할 필요가 있다고 생각했습니다. 청주페이에선 매장의 매출규모를 파악하여 등급을 부여합니다. 해당 등급이 변화한 적이 있는 매장을 파악
    
    하여, 어디에 있는 어떤 매장이 변화하였는지 파악하고자 하였습니다. 매출등급의 변화가 있던 가맹점의 처음 등급과 마지막 등급을 추출한 뒤 등급에 점수를 부여하고 그 점수의 차이로 등급의 변화를 알아보았습니다. 매출의 등급 변화를 알게 해주는 칼럼을 생성한 뒤 x,y좌표를 생성하고 QGIS로 시각화합니다.
    
    ```bash
    15) 상승, 하락 가맹점 추출
    #plot을 이용해 추세 보기
    ts.plot(mae[62])#시계열 플랏
    ts.plot(runMean(mae[734], 5))#이동평균한 플랏
    plot(lowess(mae[734]), type="l")#평활한 플랏
    ((lowess(mae[4])$y)[90]-(lowess(mae[4])$y)[1])/90
    #평활한 것의 마지막 달과 첫 번째 달의 기울기를 구한 것#원하는 매장의 매출액 추세를 확인하기 위함#기울기의 합, 수가 높을수록 지속적 상승됨을 의미
    vec=c()
    for (i in 2:46141){
     vec=append(vec, sum((lowess(mae[i])$y)[2:21]-(lowess(mae[i])$y)[1:20])) }
    #각 매장의 기울기의 합을 구함#상승폭이 큰 가맹점
    a=which(vec>6305000)
    length(a)
    b=mae[,c(a+1)]; b=names(b)
    ups=data.frame()
    for (i in 1:999){
     ups=rbind(ups, mgr %>% filter(BC_MERCHANT_CODE==b[i])) }
    ups1=ups[,c(1,3,4,6,7)]; ups1=unique(ups1); ups1=na.omit(ups1)
    mykey =""
    library("ggmap")
    register_google(key=mykey)
    geocode(location = base::enc2utf8("청주시"), source = 'google')
    ups1$MERCHANT_ADDRESS1 <- enc2utf8(ups1$MERCHANT_ADDRESS1)
    ups1=mutate_geocode(data = ups1, location = MERCHANT_ADDRESS1, source = 'google')
    #상승폭이 큰 가맹점의 주소를 이용해 (x,y)좌표를 생성
    write.csv(ups1, "C:\\Users\\user\\Desktop\\상승세매장.csv", row.names=F)
    #하락폭이 큰 가맹점
    a=which(vec< -80000)
    length(a)
    b=mae[,c(a+1)];
    b=names(b)
    downs=data.frame()
    for (i in 1:1000){
     downs=rbind(downs, mgr %>% filter(BC_MERCHANT_CODE==b[i]))
    }
    downs1=downs[,c(1,3,4,6,7)]; downs1=unique(downs1); d1=na.omit(downs1)
    downs1$MERCHANT_ADDRESS1 <- enc2utf8(downs1$MERCHANT_ADDRESS1)
    downs1=mutate_geocode(data = downs1, location = MERCHANT_ADDRESS1, source = 'google')
    #지오코딩으로 주소를 x,y좌표로 변환한 뒤 QGIS로 분석
    write.csv(downs1, "C:\\Users\\user\\Desktop\\하락세매장.csv", row.names=F)
    ```
    
    위에서 했던 분석을 파악한 결과 규모가 매우 작은 업소에도 높은 등급이 부여되는 등 데이터의 신뢰성에 의심이 가기 시작했습니다. 따라서 업소의 등급이 아닌 매출의 추세를 이용하여 다시 파악하고자 했습니다. dcast함수를 이용해 데이터 시계열로 만들어 줍니다. 시계열 데이터에서 f(b)-f(a)/(b-a) 를 이용한 기울기의 합이 높은 것은 결제액이 상승추세인 매장이라 판단할 수 있습니다. 가맹점 별 결제액 기울기의 합을 구한 다음 그 합이 큰 매장과 작은 매장을 추출합니다. 그리고 그 가맹점을 지오코딩하여 (x,y)좌표로 나타내고, qgis를 이용해 시각화 하였습니다.
    
    ```bash
    #대형마트 영향력 검정
    16) 데이터 변환
    super=dbGetQuery(con, 'SELECT * from pgr where MERCHANT_BIZ_TYPE in (4010, 4020) ')
    super=na.omit(super)
    market=dcast(super, USER_ID+CARD_NO+GENDER+USER_BIRTH_YEAR+USER_ADDRESS+age~"PAY", value.var = "PAYMENT_AMOUNT", sum)
    market=market %>% arrange(desc(PAY))
    #편의점, 슈퍼마켓에 대한 결제내역만 가져온 다음 각 회원이 해당 업종에 사용한 전체 금액을 알 수 있게 전처리
    
    17) 대형마트 반경 600m 안에 사는 사람의 영리유통점 결제금액 검정
    #단순랜덤표본 추출
    a=sample(nrow(market), 15000, replace=F)
    ransample=market[a,]
    mykey =""
    library("ggmap")
    register_google(key=mykey)
    geocode(location = base::enc2utf8("청주시"), source = 'google')
    ransample$USER_ADDRESS <- enc2utf8(ransample$USER_ADDRESS)
    ransample=mutate_geocode(data = ransample, location = USER_ADDRESS, source = 'google')
    write.csv(ransample, "C:\\Users\\user\\Desktop\\랜덤표본 위치.csv", row.names = F)
    #QGIS를 이용해 대형마트 반경 600m안에 사는 사람의 유통점 평균 결제금액을 파악
    sp=sqrt((1961*559722142223.087 + 1996*352103934991.649)/(1961+1996))
    (515876.445-375245.8327)/(sp*sqrt(1/1961+1/1996))
    #대형마트 반경 600m안에 사는 사람의 유통점 평균 결제금액이 표본 전체의 유통점 평균 결제금액 보다 작은지 검정
    ```
    
    대형마트가 근처에 사는 사람의 경우 대형마트를 이용하기 때문에 청주페이를 잘 이용하지 않을 수 있을거란 생각에 이를 확인해 보기로 하였습니다. 먼저 표본추출을 통해 결제금액의 평균을 알아보고자 하였습니다. 평균을 알기 위한 적정 표본수는 (모집단수*분산)/((모집단수-1)*(오차한계^2/4+분산)) 식으로 알 수 있습니다. 하지만 이 데이터의 경우 결제금액에 대한 분산이 너무 커서 적정 표본 수가 제대로 나오고 있지 않습니다. 따라서 임의로 전체 모집단의 10%에 해당하는 1만 5천명 정도를 임의로 추출하여 분석을 시작하였습니다,‘대형마트 주변에 사는 청주페이 이용자는 가맹점 주변에 사는 이용자
    
    보다 영리 유통점에서 사용하는 청주페이 금액이 낮을 것이다’란 가설을 세운 뒤 이를 t.test로 검증해 보았습니다. 걸어 10분 거리인 반경 600m를 기준으로 잡아 대형마트 반경 600m에 사는 이용자의 수와 그들의 결제금액, 그리고 청주페이 결제액이 상위 8개인 가맹점 주변에 사는 이용자의 수와 평균결제 금액을 비교해 보았고, 그 결과 유의하게 가맹점 주변에 사는 사람들의 유통점 평균결제 금액이 대형마트 주변에 사는 사람들의 유통점 평균결제 금액보다 높은 것으로 나타났습니다.
    
    ```bash
    17) 대형마트 반경 600m안에 사는 사람 중 영리유통점 결제금액이 높은 사람은 어떤 구매성향을 지녔는지 검정
    #10대남
    supera=dbGetQuery(con, ' SELECT * from pgr where MERCHANT_BIZ_TYPE in (4010, 4020) and age="10대" and GENDER="남 " ')
    marketa=market %>% filter(GENDER=="남 " & age=="10대")
    .......
    .......
    #70대여
    supera=dbGetQuery(con, ' SELECT * from pgr where MERCHANT_BIZ_TYPE in (4010, 4020) and age="70대" and GENDER="여 " ')
    marketa=market %>% filter(GENDER=="여 " & age=="70대")
    r=data.frame()
    for (i in 1:1000){
     a=supera %>% filter(supera$USER_ID==marketa[i,1] & supera$CARD_NO==marketa[i,2] & supera$GENDER==marketa[i,3] & supera$USER_BIRTH_YEAR==marketa[i,4] &supera$USER_ADDRESS==marketa[i,5] & supera$age==marketa[i,6])
     r=rbind(r, cbind(nrow(a),mean(a$PAYMENT_AMOUNT), sd(a$PAYMENT_AMOUNT),
     quantile(a$PAYMENT_AMOUNT, 0.25), quantile(a$PAYMENT_AMOUNT, 0.5),
     quantile(a$PAYMENT_AMOUNT, 0.75), quantile(a$PAYMENT_AMOUNT, 1)))
    }
    woman70=cbind(marketa[c(1:1000),], r)
    samsuper=rbind(man10, man20, man30, man40, man50, man60, man70,
    woman10,woman20,woman30,woman40,woman50,woman60,woman70 )
    samsuper=rename(samsuper, c("V1"="건수", "V2"="평균", "V3"="표준편차", "V4"="0.25분위", "V5"="0.5분위", "V6"="0.75분위", "V7"="1.00분위"))
    samsuper$su1=samsuper$PAY/samsuper$건수;
    samsuper$su2=samsuper$평균/samsuper$건수;
    samsuper$su3=samsuper$`0.5분위`/samsuper$건수;
    #각 성, 연령별로 결제금액이 높은 1000명씩 임의 추출하여 그들의 유통점 평균 결제금액, 결제금액분산, 결제금액 4분위수, 총 결제금액을 파악
    
    min=apply(samsuper[,c(15:17)], 2, min)
    max=apply(samsuper[,c(15:17)], 2, max)
    samsuper1=samsuper
    samsuper1=scale(samsuper[c(15:17)], center=min, scale=max-min)#정규화
    samsuper1=cbind(samsuper[,c(1:14)], samsuper1)
    library(NbClust)
    NbClust(samsuper[,c(15:17)], min.nc = 2, max.nc = 6, method = "kmeans")
    library(cluster)
    cl=pam(samsuper1[,c(15:17)], k=2, stand=F, diss=F, "euclidean")
    samsuper=cbind(samsuper, cl$clustering)
    write.csv(samsuper, "C:\\Users\\user\\Desktop\\market.csv", row.names = F)
    samsuper2=samsuper %>% filter(cl$clustering==2)
    samsuper3=samsuper %>% filter(cl$clustering==1)
    #총결제금액, 평균결제금액, 결제금액 중앙값을 결제 건수로 나눈 값으로 군집분석 시행#군집별 가맹점 매출#2군집
    r=data.frame()
    for (i in 1:5328){
     a=super %>% filter(super$USER_ID==samsuper2[i,1] & super$CARD_NO==samsuper2[i,2] & super$GENDER==samsuper2[i,3] & super$USER_BIRTH_YEAR==samsuper2[i,4] & super$USER_ADDRESS==samsuper2[i,5] & super$age==samsuper2[i,6])
     r=rbind(r, a)}
    sangjum=dcast(sang, MERCHANT_NAME+BC_MERCHANT_CODE+MERCHANT_ADDRESS1+MERCHANT_ZIP_CODE+MERCHANT_SALES_CODE+MERCHANT_BIZ_TYPE~'pay', value.var = "PAYMENT_AMOUNT", sum)
    sangjum=sangjum %>% arrange(desc(pay))
    sang=r
    #1군집
    r=data.frame()
    for (i in 1:8672){
     a=super %>% filter(super$USER_ID==samsuper3[i,1] & super$CARD_NO==samsuper3[i,2] & super$GENDER==samsuper3[i,3] & super$USER_BIRTH_YEAR==samsuper3[i,4] & super$USER_ADDRESS==samsuper3[i,5] & super$age==samsuper3[i,6])
     r=rbind(r, a)}
    sangjum1=dcast(r, MERCHANT_NAME+BC_MERCHANT_CODE+MERCHANT_ADDRESS1+MERCHANT_ZIP_CODE+MERCHANT_SALES_CODE+MERCHANT_BIZ_TYPE~'pay', value.var = "PAYMENT_AMOUNT", sum)
    sangjum1=sangjum1 %>% arrange(desc(pay))
    sang1=r
    #지오코딩#상위 가맹점
    mykey =""
    library("ggmap")
    register_google(key=mykey)
    geocode(location = base::enc2utf8("청주시"), source = 'google')
    sangjum$MERCHANT_ADDRESS1 <- enc2utf8(sangjum$MERCHANT_ADDRESS1)
    a=mutate_geocode(data = sangjum[c(1:34),], location = MERCHANT_ADDRESS1, source = 'google')
    
    write.csv(a, "C:\\Users\\user\\Desktop\\전체 상위 8개 가맹점.csv", row.names = F)
    #2군집
    mykey =""
    library("ggmap")
    register_google(key=mykey)
    geocode(location = base::enc2utf8("청주시"), source = 'google')
    samsuper2$USER_ADDRESS <- enc2utf8(samsuper2$USER_ADDRESS)
    samsuper2=mutate_geocode(data = samsuper2, location = USER_ADDRESS, source = 'google')
    write.csv(samsuper2, "C:\\Users\\user\\Desktop\\2군집 위치.csv", row.names = F)
    #1군집
    mykey =""
    library("ggmap")
    register_google(key=mykey)
    geocode(location = base::enc2utf8("청주시"), source = 'google')
    samsuper3$USER_ADDRESS <- enc2utf8(samsuper3$USER_ADDRESS)
    samsuper3=mutate_geocode(data = samsuper3, location = USER_ADDRESS, source = 'google')
    write.csv(samsuper3, "C:\\Users\\user\\Desktop\\1군집 위치.csv", row.names = F)
    #군집들의 주소를 기반으로 x,y좌표를 파악한 후 QGIS로 대형마트 주변과 인기 가맹점 사이의 군집 수 차이가 있는지 검정
    ```
    
    대형마트 주변에 살면서 영리 유통점에 결제금액이 높은 사람들이 보여 이를 확인한 결과 그들은 편의점에서 소액결제 자주 하는 것으로 드러났습니다. 대형마트에서 장을 볼 땐 한꺼번에 많은 양의 물건을 사기 때문에, 소액결제를 자주 하는 부류는 대형마트의 영향을 잘 받지 않을 것입니다. 저는 ‘청주페이 결제액이 높은 사람의 경우 대형마트 주변엔 소액결제를 자주 하는 부류가 많고, 매출액이 높은 가맹점 주변엔 한꺼번에 많은 물건을 구매하는 부류가 많을 것이다’라는 가설을 세운 뒤 이를 검증해 보기로 하였습니다. 먼저 성별, 연령별로 소액결제를 자주하는 부류와 한꺼번에 많이 구매하는 부류가 얼마나 있는지도 아는 것이 좋을거란 생각에 성별, 연령별로 결제금액이 높은 1000명을 층화 표본추출 하여 그들의 결제건수, 평균결제금액, 결제금액의 표준편차, 결제금액의 4분위수를 구해보았습니다. 그런 다음 총 결제금액, 평균 결제금액, 결제금액 중앙값을 결제 건수로 나눈 뒤 해당 값을 정규화한 뒤 군집분석을 시행하여 2개의 군집으로 나눠보았습니다. 그 결과 소액결제를 자주하는 부류와 한꺼번에 많은 물품을 구매하는 부류로 나뉘었으며, 해당 군집들의 분포가 대형마트에
    
    영향을 받는지를 알아보았습니다.
    
    이상으로 코드 리뷰를 마칩니다.
