# 쿠팡 우유 리뷰 분석

- Text: 쿠팡의 우유 리뷰 크롤링 및 텍스트 분석
- 수행 기간: 2022.09.12~2022.09.17
- 수행인원: 1
- 사용 툴: R, 파이썬
- 배경: 포트폴리오 작성을 위한  1인 개인 프로젝트입니다.

파이썬과 R을 통해 쿠팡 리뷰를 이용한 리뷰 데이터 분석을 진행합니다.

쿠팡 로켓배송 식품 판매량 랭킹 1위인 서울우유 1급A우유엔 54만개가 넘는 리뷰가 있으며, 해당 리뷰를 작성한 리뷰어의 닉네임을 클릭하면 리뷰어가 쓴 다른 상품의 리뷰 역시 볼 수 있습니다.

![image](https://github.com/user-attachments/assets/0e3ceb08-1336-4a13-903f-21d58284b4c0)

리뷰어들이 쿠팡 서울우유에서 쓴 서울우유 리뷰와 비슷한 시기 작성한 다른 상품의 리뷰들을 크롤링으로 수집해줍니다.

**크롤링**

```bash
from selenium import webdriver
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from bs4 import BeautifulSoup
import requests
import os
import urllib.request
import pyautogui
import openpyxl

wb=openpyxl.Workbook()
ws=wb.create_sheet('쿠팡우유')
ws.append(['고객', '상품', '날짜', '별점', '리뷰'])

url='https://www.coupang.com/vp/products/130180913?itemId=383114455&vendorItemId=3930090438&sourceType=CAMPAIGN&campaignId=82&categoryId=194176&isAddedCart='
browser=webdriver.Chrome("C:/chromedriver.exe")
browser.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {"source": """ Object.defineProperty(navigator, 'webdriver', { get: () => undefined }) """})
time.sleep(2)

for i in range(1,16):
    browser.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
    time.sleep(0.3)
browser.find_element(By.TAG_NAME, "body").send_keys(Keys.HOME)

time.sleep(1)
all_review_buttons=browser.find_elements(By.CSS_SELECTOR, 'span.sdp-review__article__list__info__user__name')
time.sleep(1)

for l in range(3):
    for j in range(2, 11):
        browser.find_element(By.CSS_SELECTOR, f'button.sdp-review__article__page__num:nth-child({j})').click()#2번페이지 버튼 클릭
        time.sleep(2)
        all_review_buttons=browser.find_elements(By.CSS_SELECTOR, 'span.sdp-review__article__list__info__user__name')
        time.sleep(1)
        for all_review_button in all_review_buttons:
            all_review_button.click()
            time.sleep(1.5)
            browser.find_element(By.CSS_SELECTOR, 'div.sdp-review__profile__article__count').click()
            time.sleep(0.5)
            for i in range(1,25):
                browser.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
                time.sleep(2)
            browser.find_element(By.TAG_NAME, "body").send_keys(Keys.HOME)
            custom_all_reviews=browser.find_elements(By.CSS_SELECTOR, 'div.sdp-review__profile__article__list__reviews')
            name=browser.find_element(By.CSS_SELECTOR, 'div.sdp-review__profile__article__info__name').text
            for custom_all_review in custom_all_reviews:
                time.sleep(1)
                print(name)
                print('상품')
                print(custom_all_review.find_element(By.CSS_SELECTOR, 'div.sdp-review__profile__article__list__reviews__product__name').text)
                title=custom_all_review.find_element(By.CSS_SELECTOR, 'div.sdp-review__profile__article__list__reviews__product__name').text
                print('내용')
                print(custom_all_review.find_element(By.CSS_SELECTOR, 'div.sdp-review__profile__article__list__reviews__content').text)
                contents=custom_all_review.find_element(By.CSS_SELECTOR, 'div.sdp-review__profile__article__list__reviews__content').text
                print(custom_all_review.find_element(By.CSS_SELECTOR, 'div.sdp-review__profile__article__list__reviews__star__date').text)
                date=custom_all_review.find_element(By.CSS_SELECTOR, 'div.sdp-review__profile__article__list__reviews__star__date').text
                print(custom_all_review.find_element(By.CSS_SELECTOR, 'div.sdp-review__profile__article__list__reviews__star__gray > div').get_attribute('data-rating'))
                star=custom_all_review.find_element(By.CSS_SELECTOR, 'div.sdp-review__profile__article__list__reviews__star__gray > div').get_attribute('data-rating')
                ws.append([name, title, date,  int(star), contents.strip().replace('\n','')])
            browser.find_element(By.CSS_SELECTOR, 'div.sdp-review__profile__article__close-btn').click()
            browser.find_element(By.TAG_NAME, "body").send_keys(Keys.HOME)
            time.sleep(1)
    browser.find_element(By.CSS_SELECTOR, 'button.sdp-review__article__page__next').click()
wb.save('쿠팡리뷰.xlsx')
```

총 27페이지 까지의 리뷰 데이터를 수집하였으나, 같은 리뷰어가 한 제품에 여러번 리뷰를 작성한 경우가 많았고, 중복을 제거하여 최종적으로 60명의 23448개의 리뷰를 수집하였습니다.

![image (1)](https://github.com/user-attachments/assets/ca36bf08-2179-4b8f-be50-ba257ccc730a)

크롤링으로 수집한 리뷰

**전처리**

이후 장바구니 분석을 위한 상품명 전처리를 수행합니다.

중복행을 제거하고 날짜 데이터의 형식을 날짜로 변경한 뒤 제품명의 특수문자, 영어, 숫자 등을 제거하여 제품명을 간략화 시킵니다.

```bash
origindata=pd.read_excel('C:\\Users\\me\\Desktop\\쿠팡리뷰2.xlsx', '쿠팡우유')
origindata['고객'].nunique()
origindata=origindata.drop_duplicates(['고객','상품','날짜','별점','리뷰'])

origindata['날짜']=origindata['날짜'].str.replace('.','')
origindata['날짜']=pd.to_datetime(origindata['날짜'])
origindata['날짜']=origindata['날짜'].dt.strftime('%Y%m%d')

origindata=origindata.sort_values(by=['고객', '날짜'])
origindata['상품'].nunique()
data=origindata

def cleaned_text(text):
    cleaned_text=re.sub('[-=+,#/\?:^.@*\"※~ㆍ!』‘|\(\)\[\]`\'…》\”\“\’·]','',text)
    cleaned_text=re.sub(r'\([^)]*\)','',cleaned_text)
    cleaned_text=re.sub('[0-9+][가-힣][가-힣]' , '', cleaned_text)
    cleaned_text=re.sub('[0-9+][a-zA-z][a-zA-z]' , '', cleaned_text)
    cleaned_text=re.sub('[-=+,#/\?:^.@*\"※~ㆍ!』‘|\(\)\[\]`\'…》\”\“\’㎡Ø>⁄·%_□×♥&$ㅡ㈜]','',cleaned_text)
    cleaned_text=re.sub('[0-9+][가-힣]' , '', cleaned_text)
    cleaned_text=re.sub('[0-9+][a-zA-z]' , '', cleaned_text)
    cleaned_text = re.sub('[a-zA-Z]','',cleaned_text)
    cleaned_text=re.sub(r'[0-9]+','',cleaned_text)
    cleaned_text=re.sub('  |   ','',cleaned_text)
    return cleaned_text

for i in range(23448):
    data.iloc[[i],[1]]=cleaned_text(str(data.iloc[[i],[1]])).replace('상품', '').strip()
```

![https://blog.kakaocdn.net/dn/N0r5g/btrL0S3r9Wn/jADtZkRaXEsKYF2PzKedY1/img.png](https://blog.kakaocdn.net/dn/N0r5g/btrL0S3r9Wn/jADtZkRaXEsKYF2PzKedY1/img.png)

간략화된 상품명

**연관성 분석(장바구니 분석)**

서울우유는 로켓배송 상품이기 때문에 구입을 할 경우 19800원 이상 구입을 해야 하므로 다른 상품을 같이 구매해 주어야 합니다. 서울우유를 구매한 날 같이 구매한 상품이 무엇인지 R을 통해 분석을 수행합니다.

```bash
library('readxl')
library('dplyr')
library('tidyr')
library('reshape2')
library('writexl')
library(arules)
library(KoNLP)
library(wordcloud)
data=read_excel('C:\\Users\\me\\Desktop\\data.xlsx')
data=data[,-c(4,5)]

data$고객날짜=paste(data$고객, data$날짜, sep="-")
data$고객날짜=as.factor(data$고객날짜)
data$상품=as.factor(data$상품)
data$고객=as.factor(data$고객)
data$날짜=as.factor(data$날짜)

tran=as(split(data$상품, data$고객날짜), "transactions")
ap=apriori(data=tran, parameter = list(support=0.3, confidence=0.7, minlen=2))
summary(ap); head(inspect(ap), 20)

tran=as(split(data$상품, data$고객), "transactions")
ap=apriori(data=tran, parameter = list(support=0.3, confidence=0.7, minlen=2))
summary(ap); head(inspect(ap), 20)

tran=as(split(data$상품, data$날짜), "transactions")
summary(ap); head(inspect(ap), 20)
ap=apriori(data=tran, parameter = list(support=0.3, confidence=0.7, minlen=2))
```

장바구니 분석을 할 때 고객과 날짜를 모두 고려하여 한 고객이 그 날 구입한 물건들을 1개의 장바구니로 구분하려 했으나, 고객과 날짜를 모두 고려한 것과 날짜를 고려한 장바구니는 물품들의 연관성이 적게 나와 유의미한 결론을 도출할 수 없었습니다.

따라서 날짜를 고려하지 않고, 각 고객이 구입하여 리뷰를 작성한 물건들을 한개의 장바구니로 구분하여 지지도와 향상도를 파악한 결과, [곰곰]브랜드의 [콩두부, 우유식빵, 아삭한 콩나물]과 국내산 애호박이 높은 지지도와 향상도를 보임을 파악할 수 있었습니다.

![https://blog.kakaocdn.net/dn/uiUHN/btrL3Fh2an7/4VofEmHHnv6cZtWzQKqPZK/img.png](https://blog.kakaocdn.net/dn/uiUHN/btrL3Fh2an7/4VofEmHHnv6cZtWzQKqPZK/img.png)

서울우유를 구입한 적이 있는 고객(즉 크롤링 된 전체 리뷰어)의 1/3은 콩두부와 우유식빵, 콩나물, 애호박을 구입한 적이 있다.

하지만 날짜를 고려하지 않고 고객별 장바구니만을 분석한 연관성 분석은 큰 의미를 갖지 않는다고 생각하여, 고객과 날짜를 모두 고려한 장바구니 중에 서울우유를 구입한 고객이 많이 구입한 물건이 무엇인지 직접 파악하기로 하였습니다.

```bash
data['상품'].str.contains('서울우유 우유', na = False)
data_milk=data[origindata['상품'].str.contains('서울우유 우유')]
data_milk['고객'].nunique()

milk_day=pd.DataFrame()
for i in range(0, 58):
    a=pd.merge(data, data_milk[data_milk['고객']==data_milk['고객'].unique()[i]].iloc[:,[0,2]], how='right', on=['고객','날짜'] )
    milk_day=pd.concat([milk_day, a])

milk_day['상품'].nunique()
milk_bag=milk_day.drop_duplicates(['고객','상품','날짜'])
milk_bag=milk_bag[['고객','상품','날짜']]
#서울우유를 구매한 날 구입한 물건만 나타내는 데이터 프레임을 작성하는 파이썬 코드

milk_bag=read_excel('C:\\Users\\me\\Desktop\\milkbag.xlsx')
bag_combine=dcast(milk_bag, 고객+날짜~상품, value.var = "상품")
bag_combine=bag_combine[,-c(1,2)]#고객, 날짜 열을 제거
bag_combine[is.na(bag_combine) == FALSE]=1
bag_combine[is.na(bag_combine) == TRUE]=0

for (i in 1:1432){
  bag_combine[,i]=as.integer(bag_combine[,i])}

tail(sort(apply(bag_combine, 2, sum)), 20)
#서울우유 우유를 구입한 사람이 같은 장바구니에 무엇을 많이 담았는지 확인하는 R코드
```

날짜와 고객을 모두 고려한 결과 고객들이 서울우유를 구입한 적이 있는 날은 총 319번이며,

같이 구매한 물건은 삼립저온숙성 탕종 숙식빵이 11번, 곰곰 국내산 백오이가 10번, 서울우유 더 진한 플레인 요거트가 9번으로, 가장 연관성이 높은 탕종 숙식빵과 서울우유의 지지도는 11/319=0.03으로 그 값이 매우 낮게 나오는 것을 알 수 있습니다.

허나 전반적으로 봤을 때, 우유를 구입한 사람들은 같은 장바구니에 채소류와 빵류를 주로 같이 담고 있음을 파악할 수 있습니다.

**리뷰 데이터 분석**

장바구니 분석을 수행하였으니 리뷰 데이터를 분석해 보도록 합니다.

위 데이터를 확인해 본 결과 별점 5점의 리뷰가 압도적으로 많고, 또한 서울우유의 리뷰는 319개 밖에 되지 않아 다양한 별점의 우유 리뷰를 150페이지 까지 크롤링 한 뒤 분석합니다.

**별점별 리뷰 크롤링**

```bash
from selenium import webdriver
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from bs4 import BeautifulSoup
import requests
import os
import urllib.request
import pyautogui
import openpyxl

wb=openpyxl.Workbook()
ws=wb.create_sheet('쿠팡우유 별점리뷰')
ws.append(['별점', '리뷰'])

url='https://www.coupang.com/vp/products/130180913?itemId=383114455&vendorItemId=3930090438&sourceType=CAMPAIGN&campaignId=82&categoryId=194176&isAddedCart='
browser=webdriver.Chrome("C:/chromedriver.exe")
browser.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {"source": """ Object.defineProperty(navigator, 'webdriver', { get: () => undefined }) """})
browser.get(url)
time.sleep(2)

for i in range(1,10):
    browser.find_element(By.TAG_NAME, "body").send_keys(Keys.END)#쿠팡 서울우유 페이지의 스크롤을 전부 내린다
    time.sleep(0.3)
browser.find_element(By.TAG_NAME, "body").send_keys(Keys.HOME)#쿠팡 서울우유 페이지의 스크롤을 올린다
time.sleep(2)
browser.find_element(By.TAG_NAME, "body").send_keys(Keys.PAGE_DOWN)
browser.find_element(By.TAG_NAME, "body").send_keys(Keys.PAGE_DOWN)
time.sleep(1)

browser.find_element(By.CSS_SELECTOR, 'div.sdp-review__article__order__star__all__current').click()#별점 보기 클릭
time.sleep(2)
for i in range(1, 6):#1점~5점 출력
    time.sleep(1)
    browser.find_element(By.CSS_SELECTOR, f'div.sdp-review__article__order__star__list__item__star__graph--{i}').click()#별점을 i점 클릭
    time.sleep(1)
    browser.find_element(By.TAG_NAME, "body").send_keys(Keys.HOME)#쿠팡 서울우유 페이지의 스크롤을 올린다
    time.sleep(0.5)
    for l in range(15):#아래 루프를 10번 반복 (넥스트 페이지를 10번 누르기 위함)
        time.sleep(1)
        for j in range(2, 11):#페이지 버튼 1번 부터 10번 클릭하기 위함
            time.sleep(0.5)
            browser.find_element(By.TAG_NAME, "body").send_keys(Keys.HOME)#쿠팡 서울우유 페이지의 스크롤을 올린다
            time.sleep(1)
            browser.find_element(By.CSS_SELECTOR, f'button.sdp-review__article__page__num:nth-child({j})').click()#j-1번째 페이지 클릭
            time.sleep(3)
            milk_reviews=browser.find_elements(By.CSS_SELECTOR, 'div.sdp-review__article__list__review__content')#리뷰를 선택
            time.sleep(1)
            for r in range(len(milk_reviews)):
                contents=milk_reviews[r].text#리뷰의 본문을 크롤링print(contents)
                ws.append([int(f'{i}'), contents.strip().replace('\n',' ')])#크롤링 한 내용을 엑셀에 저장
                time.sleep(1)
        browser.find_element(By.CSS_SELECTOR, 'button.sdp-review__article__page__next').click()
        time.sleep(2)
    browser.find_element(By.CSS_SELECTOR, f'div.sdp-review__article__order__star__all__current__star__graph--{i}').click()#별점 보기를 다시 클릭
    time.sleep(2)
wb.save('쿠팡리뷰 별점별.xlsx')
```

```bash
star_review=pd.read_excel('C:\\Users\\me\\Desktop\\쿠팡리뷰 별점별.xlsx')
for i in range(1,6) :   print(len(star_review[star_review['별점']==i]))#총 150페이지의 리뷰를 크롤링한 결과
```

150페이지 까지 크롤링 한 결과, 1점 리뷰는 614개, 2점 리뷰는 440개, 3점 리뷰는 478개, 4점 리뷰는 675개, 5점 리뷰는 674개가 나타났습니다. 리뷰 점수별 크롤링 된 리뷰 수가 차이가 나는데, 글자 리뷰를 남기지 않고 별점 리뷰만 남긴 경우가 많고 2, 3점의 경우 오래된 리뷰일수록 대부분이 별점만 남긴 리뷰였습니다.

**리뷰데이터 정제**

```bash
star_review['리뷰'] = star_review['리뷰'].str.replace("[^가-힣 ]","")#한글 이외 문자 제거
def review_cleaned(text):
    review_cleaned=re.sub('제 리뷰읽고 좋은일 생겼다는 분들 많아서 좋네요', '', text)
    review_cleaned=re.sub('진짜 진짜 제품이 좋은거 같아요최고입니다 지인들이 추천하길래 구매하게되었는데요이제서야 이 제품을 알았다는게 후회되네요정말 한번 구매하면 계속 구매하게될거같아요만족도는 너무너무 좋습니다다른분들에게도 추천했는데 다 좋다고 하네요재구매의사 너무나도 있고요|마지막으로 읽어주셔서 감사하며도움되셨다면 추천 한번씩 부탁드릴게요좋은일들 많으시고 행복하시길 기하겠습니다감사합니다|너무너무 좋은 제품이고요만족합니다 추천드려요마지막으로  읽어주셔서 감사하며|제 리뷰 읽고 좋은일 생겼다는 분들 많아서 좋네요|도움이 되신다면 살포시 도움 눌러주세요|제 가 도움이 되었다면 도움이돼요를 눌러주세요|도움이 돼요 눌러주신분들은 건강하시고 행복하세요|후기 작성에 앞서 작성자 입맛 취향으로 매우 주관적인 판단으로 적은 후기입니다|인터넷 쇼핑을 많이 하는 사람으로서 후기를 참고하는 경우가 많습니다|인터넷 쇼핑을 좋아하고 자주 하는 사람으로서 다른 분들의 후기를 참고하는 경우가 많습니다|인터넷 쇼핑을 많이 하는 사람으로서 후기를 참고하는 경우가 많습니다|인터넷 쇼핑이 취미인 사람으로서 후기를 참고하는 경우가 많습니다|저도 제품 검색하시는 분께 작은 도움이라도 되길 바라는 마음으로 작성하고 있습니다|저도 다른 분께 도움이 되었으면 하는 마음에서 작성하고 있습니다|안녕하세요 소비의여신입니다제가 쓰는 후기가 이 상품을 구매하려는 분들께 조금 이나마 도움이 되었으면 하는 바램으로 몇 글자 적어봅니다|안녕하세요 소비의여신입니다제가 쓰는 후기가 이 상품을 구매하려는 분들께 조금이나마 도움이 되었으면 하는 바램으로 몇 글자 적어봅니다|안녕하세요 소비의 여신입니다.제 후기가 다른분들께 조금이나마 도움이 되었으면 하는 바램으로 몇 글자 적어봅니다|제 후기가 도움이 되었다면 도움이 돼요를 부탁드려요','', review_cleaned)
    review_cleaned=re.sub('내돈내산 찐후기제 리뷰읽고 좋은일 생겼다는 분들 많아서 좋네요', '', review_cleaned)
    review_cleaned=re.sub('너무너무 만족합니다.주변에서 추천하길래 한번 사보았는데요제품이 너무 만족스럽고 좋은거같아요그래서 이제는 이거만 쓸려고요 대량으로 구매도 해볼까 생각중입니다정말 이건 사봐야알거같아요제품패키지나 포장 배송 너무 만족합니다이제야 최애제품을 찾은거 같아 좋습니다 배송역시 쿠팡이네요 정말 빠르게 도착하였씁니다 포장꼼꼼하게 잘 포장되어서 도착하였습니다 패키지너무 괜찮네요 이쁘고 가성비너무 만족스럽니다.이정도에 이가격이라 정말 가성비가 좋아요 총평너무 만족하고 다음에도 또 구매할거예요!추천드립니다마지막으로 제 리뷰가 도움이되셨다면추천 한번씩 부탁드릴게요감사합니다', '', review_cleaned)
    review_cleaned=re.sub('직접 사서 직접 써보고좋은점 나쁜점 잘 구분해서 를 작성하는 인입니다', '', review_cleaned)
    review_cleaned=re.sub('제 상품평이 여러분의 구매 선택에 도움이 되길 희망합니다 구매동기', '', review_cleaned)
    review_cleaned=re.sub('코로나도 다시 기승이라 직접 사러 가고 싶진 않고더운날 나가지 않아도 되고 간편하게 집에서 먹으려 주문했어요', '', review_cleaned)
    review_cleaned=re.sub('쓰면서 편했던 점 위주로 올리려고 노력하는 중입니다.딴분들의 후기가 제게 도움 됬듯이, 제 후기도 도움 되시기를 바랍니다지극히 개인적 취향이 반영된 후기이므로, 편하게 읽어주셔요︎', '', review_cleaned)
    review_cleaned=re.sub('직접 사서 직접 써보고,좋은점 나쁜점 잘 구분해서 후기를 작성하는 1인입니다.무조건 좋은 말만 올리지 않고', '', review_cleaned)
    review_cleaned=re.sub('상기 내용 처럼 매우 까다롭고 까탈스런 입맛을 바탕으로 작성된 후기임을 다시한번 알려드립니다', '', review_cleaned)
    review_cleaned=re.sub('기준 구매가 원본격 후기 작성에 앞서 작성자 입맛 취향으로 매우 주관적인 판단으로 적은 후기입니다본격 리뷰', '', review_cleaned)
    review_cleaned=re.sub('기준 구매가 본격 후기 작성에 앞서 작성자 입맛 취향으로 매우 주관적인 판단으로 적은 후기입니다본격 리뷰', '', review_cleaned)
    review_cleaned=re.sub('미안하다 이거 보여주려고 어그로 끌었다', '', review_cleaned)
    review_cleaned=re.sub('고민끝에 이 제품을 샀습니다너무 좋습니다 제품 자체는 뭐라고 할거없이 너무 만족스럽고요꼼꼼한 포장에 또한번 놀랐습니다제품의 퀄이 너무 좋고 만족스럽네요대량으로 사놓고 싶습니다재구매율 할만합니다너무 감사합니다빠른배송에 감사하고좋은 제품에 감사합니다', '', review_cleaned)
    review_cleaned=re.sub('제 리뷰가 도움이 되셨다면 도움이돼요 한번씩 부탁드릴게요건강하시고 로또 당첨되시고 하시는일 잘되시고행복하시길 기원하고 응원하겠습니다감사합니다', '', review_cleaned)
    review_cleaned=re.sub('다른분에게 도움이 되고자 솔직하게 작성한 리뷰입니다', '', review_cleaned)
    review_cleaned=re.sub('제  글이  조금이라도  도움이  되셨다면  좋겠습니다  유익하셨다면  도움이 돼요  한 번  눌러주세요', '', review_cleaned)
    review_cleaned=re.sub('제 상품평이 여러분의 구매 선택에 도움이 되길 희망합니다구매동기우리 애들이 너무 좋아하는 토마토 맛도 좋고 영양도 좋아 자주 시켜 먹고 있어요 코로나도 다시 기승이라 직접 사러 가고 싶진 않고더운날 나가지 않아도 되고 간편하게 집에서 먹으려 주문했어요실사용후기로켓배송은 언제나 빠르고 정확해서 좋아요저렴한 가격이라 기대 안했는데 맛있어서 맘에 들어요저희 애들은 토마토 너무 좋아해서 자주 주문해서 먹고 있어요간식으로도 간편하고 너무 좋아요 애들도 너무 맛있다하며 좋아해요 생각보다 신선하고 맛있어요편하게 집에서 주문하고 배송받아 편하게 먹을수 있는게 좋아요총평부담스럽지 않은 가격에 맛도 있어서 자주 구매하고 싶어요애들이 맛있다며 좋아해서 추천합니다  ', '', review_cleaned)
    review_cleaned=re.sub('제 상품평이 여러분의 구매 선택에 도움이 되길 희망합니다구매동기', '', review_cleaned)
    review_cleaned=re.sub('솔직 후기입니다구매시 도움이 되길 바랍니다', '', review_cleaned)
    review_cleaned=re.sub('도움이 되셨기를 바라고요  오늘 하루도 즐겁고 행복하게 보내세요 ', '', review_cleaned)
    review_cleaned=re.sub('안녕하세요 소비의 여신입니다제가 쓰는 후기가 여러분들께 조금이나마 도움이 되었으면 하는바램으로 몇자 적어봅니다', '', review_cleaned)
    review_cleaned=re.sub('안녕하세요  언제나  솔직한  상품평을  쓰는  김지영입니다', '', review_cleaned)
    review_cleaned=re.sub('제품을 검색하시는 분께 조금이라도 도움이 되고자 작성하는 솔직한 후기입니다', '', review_cleaned)
    review_cleaned=re.sub('인터넷 쇼핑이 취미인 사람으로서 후기를 참고하는 경우가 많습니다저도 다른 분께 도움이 되었으면 하는 마음에서 작성하고 있습니다', '', review_cleaned)
    review_cleaned=re.sub('리뷰 읽어주셔서 감사하며 항상 해복하시고 좋은일 생기시길 바라겠습니다리뷰 추천 한번씩 눌러주시면 감사합니다|직접 사서 직접 써보고좋은점 나쁜점 잘 구분해서 후기를 작성하는 인입니다|다른분들에게 도움이 되고자 |다른 분들에게 도움이 되고자|솔직하게|체험하고|직접 사용또는 시식해보고|실제사용해보고 먹어보고|실제사용해보고 먹어보고|실제 사용하고 작성한|작성한 솔직한 리뷰입니다|쿠팡체험단 이벤트로 상품을 무료로 제공 받아 작성한 구매 후기입니다', '', review_cleaned)
    review_cleaned=re.sub('실화냐진짜 내가 감격스럽고 부터가슴울리는 장면들이 뇌리를 스치면서 가슴이 감격해진다진짜 감격스럽고 이제품을 최근에 알았는데미안하다내가 다 뭔가 알수없는 추억이라 해야되나 그런감정이 이상하게 얽혀있다진짜팩트냐진짜 개충격먹어가지고 와 소리 저절로 나오더라진짜 이거 개오지데와 진짜 이거이냐옛날생각나고 나 중딩때생각나고 뭔가 슬프기도 하고 좋기도 하고 감격도 여러가지 감정이 복잡하네아무튼 진짜 최고명작임|다른분에게 도움이되고자  작성한 리뷰입니다|다른분에게 도움이 되고자  작성한 리뷰입니다|실제 사용하거나 시식해보고  작성한 리뷰입니다|사용해보고 작성한 리뷰입니다|후기를 귀찮아도 적는것은 다음번 구매에 도움이 되고자 하며좋은것이든 나쁜것이든 공유를 하면 좋을거 같아서입니다', '', review_cleaned)
    review_cleaned=re.sub('감사하고좋은 제품에 감사합니다', '', review_cleaned)
    review_cleaned=re.sub('저도 제품 검색하시는', '', review_cleaned)
    review_cleaned=re.sub('구매일 당시 구매가 원분류', '', review_cleaned)
    review_cleaned=re.sub('내돈내산 솔직 후기입니다','', review_cleaned)
    return review_cleaned

for i in range(2881):
    star_review.iloc[[i],[1]]=review_cleaned(str(star_review.iloc[[i],[1]])).replace('리뷰', '').strip()
star_review['리뷰'] = star_review['리뷰'].str.replace("[^가-힣 ]","")#한글 이외 문자 제거
```

5점짜리 리뷰에선 많은 수의 리뷰를 남기는 리뷰어가 처음과 끝에 인삿말을 남기는 경우가 많습니다.

해당 인삿말은 불필요한 정보를 담고 있어 제거합니다.

**워드클라우드**

```bash
for i in range(1,6):
    okt = Okt(); noun=[]
    globals()[str(i)+'_review']=star_review[star_review['별점']==i]
    for j in range(len(globals()[str(i)+'_review'])):
        nouns = okt.nouns(str(globals()[str(i)+'_review'].iloc[[j],[1]]))
        noun=noun+nouns
    words = [n for n in noun if len(n) > 1]# 단어의 길이가 1개인 것은 제외
    c = Counter(words)# 위에서 얻은 words를 처리하여 단어별 빈도수 형태의 딕셔너리 데이터를 구함
    wc = WordCloud(font_path='malgun', width=1000, height=1000, scale=5.0, max_font_size=500)
    gen = wc.generate_from_frequencies(c)
    plt.figure(figsize=(15,15))
    plt.imshow(gen)
```

![https://blog.kakaocdn.net/dn/bVfPSO/btrMjRhvDBv/K2NKJve5cZRKlpS5ZKzRY1/img.png](https://blog.kakaocdn.net/dn/bVfPSO/btrMjRhvDBv/K2NKJve5cZRKlpS5ZKzRY1/img.png)

왼쪽 위에서 부터 오른쪽 아래까지 순서대로 1점~5점까지 리뷰의 워드 클라우드입니다.

1,2점의 경우 실망, 임박, 하루,이틀이란 단어가 눈에 보이며, 4,5점의 경우 역시, 자주라는 단어가 눈에 띕니다.

전반적으로 유통기한이라는 단어가 눈에 띄는데 각각 무슨 의미를 담고 있는지 word2vec으로 파악합니다.

**word2vec**

```bash
for i in range(1,6):
    okt = Okt(); tokenized_data = []
    for sentence in tqdm(globals()[str(i)+'_review']['리뷰']):
        tokenized_sentence = okt.nouns(sentence)# 토큰화
        stopwords_removed_sentence = [word for word in tokenized_sentence if not word in [n for n in tokenized_sentence if len(n) == 1]]#길이가 1인 단어 제거
        tokenized_data.append(stopwords_removed_sentence)
    globals()['model'+str(i)] = Word2Vec(sentences = tokenized_data, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)

for i in range(1,6):    print(re.sub("[^가-힣 ]", '',str(globals()['model'+str(i)].wv.most_similar('우유'))).replace('  ', '\n') ,i)
```

| 배송포장주문기한박스유통제품그냥구매쿠팡 | 유통배송기한주문쿠팡상품반품프레날짜일주일 | 제품배송기한마트유통리터주문쿠팡포장비닐 | 자주가격마트배송정도구매유통아침때문기한 | 아이시리얼자주간식요즘한잔먹기사용제일하루 |
| --- | --- | --- | --- | --- |

우유와 함께오는 단어를 출력했을 때 결과입니다. 왼쪽부터 1점~5점 순서입니다.

별점이 높을수록 [제일, 먹기, 자주, 구매, 정도]와 같이 긍정적인 단어의 수가 많이 보이는 반면

별점의 수가 낮을 수록 [반품, 그냥]과 부정적 단어가 많이 보이며 유통 기한에 대한 언급이 많아집니다.

그렇다면 사람들이 무엇에 실망하였는지 '실망,'반품','임박'으로 검색해 봅시다.

| 우유주문구매배송포장상품새벽그냥기한유통 | 우유유통기한프레쿠팡주문배송이번날짜재고 | 유통리터와우구매그냥주문비닐우유배송기한 |  |  |
| --- | --- | --- | --- | --- |
| 우유쿠팡포장배송로켓제품처음기한그냥주문 | 우유유통배송주문기한쿠팡이번일주일월일상품 | 유통비닐우유처음리터마트와우월일날짜프레 |  |  |
| 배송우유유통기한확인프레마트상품주문리터 | 유통우유배송기한재고이번월일날짜쿠팡가격 | 제품우유주문유통배송월일마트하나쿠팡날짜 |  | 저렴가격식빵쿠폰그것집앞천원죠리퐁일주일무난 |

왼쪽부터 1~5점, 위에서부터 실망, 반품, 임박으로 검색했을 경우입니다.

높은 점수인 4점, 5점의 경우 실망이나 반품 같은 단어가 나오지 않고 있지만, 1,2,3점에선 공통적으로 유통, 배송, 기한 등의 단어와 함께 오고 있습니다. 좀 더 명확하게 보기 위해 시각화를 수행합니다.

```bash
for i in range(1,6):
    word_vectors = globals()['model'+str(i)].wv
    vocabs=globals()['model'+str(i)].wv.index_to_key
    word_vectors_list = [word_vectors[v] for v in vocabs]
    xys = PCA(n_components=2).fit_transform(word_vectors_list)[1:80,:]#단어가 잘 안보이니 80개만 생성
    xs = xys[:,0]
    ys = xys[:,1]
    plt.rcParams['font.family'] = 'Malgun Gothic'
    plt.figure(figsize=(12, 8))
    plt.scatter(xs, ys, marker = 'o')
    for j,v in enumerate(vocabs):
        plt.annotate(v, xy=(xs[j], ys[j]))
```

![https://blog.kakaocdn.net/dn/ea6xww/btrMnLWyMZm/BzXfSve182LF8X3VrVWjuk/img.png](https://blog.kakaocdn.net/dn/ea6xww/btrMnLWyMZm/BzXfSve182LF8X3VrVWjuk/img.png)

1점, 2점

![image (2)](https://github.com/user-attachments/assets/d81cae7a-ced8-44bc-90db-35638d6fae38)

3점, 4점

![https://blog.kakaocdn.net/dn/QutW0/btrMrSUUVS4/ynWBUOWR6U4Klf0z7TTBLK/img.png](https://blog.kakaocdn.net/dn/QutW0/btrMrSUUVS4/ynWBUOWR6U4Klf0z7TTBLK/img.png)

5점

시각화 결과 유사단어를 출력했을 때 보다 좀 더 쉽게 이해할 수 있게 되었습니다.

1점의 경우 [매번, 지네, 이용], [항상, 매번, 일자], [배달, 상태, 임박] 등의 단어가 함께 오며,

2점의 경우 [도착,파손,임박,반품], [상태,재고,생각,도착], [실망,계속,확인,오늘] 같은 부정적 단어가 함께 붙어 눈에 띕니다.

4,5점의 경우 [역시, 매일, 항상, 이용] 등의 단어가 함께 붙으며 부정적 단어는 눈에 띄지 않습니다.

한글의 경우 리뷰가 표준문자가 아닐 경우, 예를 들어 고소하다를 꼬소하다라고 적을 시 위 두 단어는 서로 다른 것으로 인식됩니다. 번역기는 위 두 단어를 같은 단어로 인식하므로, 번역을 수행한 뒤 분석하면 결과가 달라지는지 확인합니다.

**영문으로 번역**

```bash
pip install googletrans==4.0.0-rc1
#pip install pypapago#from pypapago import Translator
import googletrans

star_review['영문리뷰']=star_review['리뷰']
translator=googletrans.Translator()
#translator = Translator()for i in range(len(star_review)):
    star_review['영문리뷰'][i]=translator.translate(star_review['리뷰'][i], dest='en')
```

위 코드를 이용하면 쉽게 한글을 영어로 번역할 수 있지만, 대용량 데이터를 위 방법으로 번역 사이트에서 분석가의 ip를 차단하여 번역을 불가능 하게 만듭니다.

위 방법이 아닌 크롤링을 이용하여 번역을 수행합니다.

```bash
#구글
url='https://translate.google.co.kr/?hl=ko&sl=auto&tl=ko&op=translate'
browser=webdriver.Chrome("C:/chromedriver.exe")
browser.get(url)
time.sleep(1)
browser.find_elements(By.CSS_SELECTOR, 'span.VfPpkd-YVzG2b')[5].click()
time.sleep(1)

for i in range(len(star_review)):
    clipboard.copy(star_review['리뷰'][i])
    browser.find_element(By.CSS_SELECTOR, 'textarea.er8xn').click()
    ActionChains(browser).key_down(Keys.CONTROL).send_keys('v').key_up(Keys.CONTROL).perform()
    time.sleep(3)
    star_review['영문리뷰'][i]=browser.find_element(By.CSS_SELECTOR, 'span.Q4iAWc').text
    time.sleep(1)
    browser.find_element(By.CSS_SELECTOR, 'div.DVHrxd').click()
###############################################################################피파고
url='https://papago.naver.com/'
browser=webdriver.Chrome("C:/chromedriver.exe")
browser.get(url)
time.sleep(1)

for i in range(2327,len(star_review)):
    clipboard.copy(star_review['리뷰'][i])
    browser.find_element(By.CSS_SELECTOR, 'div#sourceEditArea').click()
    ActionChains(browser).key_down(Keys.CONTROL).send_keys('v').key_up(Keys.CONTROL).perform()
    time.sleep(5)
    star_review['영문리뷰'][i]=browser.find_element(By.CSS_SELECTOR, 'div#txtTarget > span').text
    time.sleep(1)
    browser.find_element(By.CSS_SELECTOR, 'button.btn_text_clse___1Bp8a').click()

star_review.to_excel('C:\\Users\\me\\Desktop\\star_review.xlsx', index=False)
```

위는 구글 번역기를 이용해 번역을 하는 코드이며, 아래는 파파고를 이용해 번역하는 코드입니다.

번역 결과 구글의 경우 장문의 글의 일부 문장을 생략하여 번역하는 경우가 있어, 파파고 번역기를 이용하였습니다.

### **워드클라우드**

```bash
import numpy as np
import pandas as pd
from datetime import datetime
from datetime import datetime, timedelta
import pymysql
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import collections
from konlpy.tag import Twitter
from konlpy.tag import Okt
from wordcloud import WordCloud
from collections import Counter
from PIL import Image
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from tqdm import tqdm
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.lancaster import LancasterStemmer
from nltk.stem import WordNetLemmatizer
pd.options.display.max_colwidth = 1500

#영어 리뷰 분석
star_review=pd.read_excel('C:\\Users\\me\\Desktop\\star_review.xlsx')
#전처리for i in range(2881):   star_review['영문리뷰'][i] = re.sub('[^a-zA-Z]', ' ', str(star_review['영문리뷰'][i]))#영어 이외 제거for i in range(2881):   star_review['영문리뷰'][i]=star_review['영문리뷰'][i].lower()#소문자화#스테밍(어간추출)
lancaster_stemmer = LancasterStemmer()
for i in range(len(star_review)):   lancaster_stemmer.stem(star_review['영문리뷰'][i])

for i in range(1,6):    globals()[str(i)+'_review']=star_review[star_review['별점']==i]#별점별 나누기#토큰화for i in range(1,6):
    globals()[str(i)+'_word']=[]
for i in range(1,6):
    for j in range(len(globals()[str(i)+'_review'])):
        globals()[str(i)+'_word']=globals()[str(i)+'_word']+star_review['영문리뷰'][j].split()#토큰화

for i in range(1,6):    globals()[str(i)+'_word'] = [w for w in globals()[str(i)+'_word'] if not w in stopwords.words('english')]#불용어제거#레마타제이션(동음이의어 판단)
wordnet_lemmatizer = WordNetLemmatizer()
for i in range(1,6):
    globals()[str(i)+'_word'] = [wordnet_lemmatizer.lemmatize(w) for w in globals()[str(i)+'_word']]
```

```bash
#워드클라우드for i in range(1,6):
    words = [n for n in globals()[str(i)+'_word'] if len(n) > 1]# 단어의 길이가 1개인 것은 제외
    c = Counter(words)# 위에서 얻은 words를 처리하여 단어별 빈도수 형태의 딕셔너리 데이터를 구함
    wc = WordCloud(font_path='malgun', width=1000, height=1000, scale=5.0, max_font_size=500)
    gen = wc.generate_from_frequencies(c)
    plt.figure(figsize=(15,15))
    plt.imshow(gen)
```

![image (3)](https://github.com/user-attachments/assets/25a7fdf6-0957-49e3-8943-6f19dc9b3900)

영문으로 번역한 리뷰를 워드클라우드 한 결과입니다.

```bash
##word2vec 전처리
star_review=pd.read_excel('C:\\Users\\me\\Desktop\\star_review.xlsx')
for i in range(2881):   star_review['영문리뷰'][i] = re.sub('[^a-zA-Z]', ' ', str(star_review['영문리뷰'][i]))#영어 이외 제거for i in range(2881):   star_review['영문리뷰'][i]=star_review['영문리뷰'][i].lower()#소문자화for i in range(1,6):    globals()[str(i)+'_review']=star_review[star_review['별점']==i]#별점별 나누기#토큰화+불용어제거+스테밍+레마타제이션
okt = Okt(); stemmer = nltk.stem.PorterStemmer(); wordnet_lemmatizer = WordNetLemmatizer()
for i in range(1,6):
    globals()[str(i)+'_word']=[]

for i in range(1,6):
    for sentence in tqdm(globals()[str(i)+'_review']['영문리뷰']):
        tokenized_sentence=okt.morphs(sentence)#토큰화
        tokenized_sentence=[w for w in tokenized_sentence if not w in stopwords.words('english')]#불용어제거for j in range(len(tokenized_sentence)):
            tokenized_sentence[j]=stemmer.stem(tokenized_sentence[j])#스테밍
            tokenized_sentence=[wordnet_lemmatizer.lemmatize(w) for w in tokenized_sentence]#레마타제이션
        tokenized_data=[word for word in tokenized_sentence]
        globals()[str(i)+'_word'].append(tokenized_data)

#word to vec 모델적합for i in range(1,6):
    globals()['model'+str(i)] = Word2Vec(sentences=globals()[str(i)+'_word'], vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)
for i in range(1,6):    print(re.sub("[^A_Za-z ]", '',str(globals()['model'+str(i)].wv.most_similar('expir'))).replace('  ', '\n') ,i)

#시각화for i in range(1,6):
    word_vectors = globals()['model'+str(i)].wv
    vocabs=globals()['model'+str(i)].wv.index_to_key
    word_vectors_list = [word_vectors[v] for v in vocabs]
    xys = PCA(n_components=2).fit_transform(word_vectors_list)#단어가 잘 안보이니 50개만 생성
    xs = xys[:,0]
    ys = xys[:,1]
    plt.rcParams['font.family'] = 'Malgun Gothic'
    plt.figure(figsize=(12, 8))
    plt.scatter(xs, ys, marker = 'o')
    for j,v in enumerate(vocabs):
        plt.annotate(v, xy=(xs[j], ys[j]))
```

![https://blog.kakaocdn.net/dn/SS3Fi/btrMjX5epjs/BSTP0w8Bwri6b0ESRee3e0/img.png](https://blog.kakaocdn.net/dn/SS3Fi/btrMjX5epjs/BSTP0w8Bwri6b0ESRee3e0/img.png)

word2vec을 시각화 한 결과입니다. 전반적으로 단어들이 떨어져 있고 어떤 연관성이 있는지 한눈에 들어오지 않으므로 군집화 시켜 확인합니다.

wv.most_similar 함수로 특정 단어와 함께 오는 단어를 출력할 경우 빈도수가 높은 단어 위주로 나오는 것으로 판단됩니다. 따라서 군집분석으로 위 시각화 이미지의 단어들을 군집화 한 뒤 어떤 단어들이 서로 같이 나오는지 보겠습니다.

**군집분석**

```bash
for i in range(1,6):
    word_vectors = globals()['model'+str(i)].wv
    vocabs=globals()['model'+str(i)].wv.index_to_key
    word_vectors_list = [word_vectors[v] for v in vocabs]
    xys = PCA(n_components=2).fit_transform(word_vectors_list)
    a=pd.concat([pd.DataFrame(vocabs), pd.DataFrame(xys)], axis=1)
    a.to_excel(f'C:\\Users\\me\\Desktop\\a{i}.xlsx', index=False)
```

word2vec으로 각 좌표가 표시되었습니다. 단어와 그 단어의 좌표를 데이터 프레임의 형식으로 만든 뒤 엑셀파일로 저장합니다.

```bash
library('readxl')
for (i in 1:5){
  assign(paste0("a",i), (read_excel(paste0('C:\\Users\\me\\Desktop\\a', i, '.xlsx'))))}

library(NbClust)
for (i in 1:5){
  NbClust(get(paste0("a",i))[,c(2,3)], min.nc = 2, max.nc = 30, method = "kmeans")}

library(cluster)
for (i in 1:5){
  cl=pam(get(paste0("a",i))[,c(2,3)], k=29, stand=F, diss=F, "euclidean")
  assign(paste0("c",i),cbind(get(paste0("a",i)), cl$clustering))}
```

해당 엑셀파일을 R로 가져와 군집분석을 시작합니다. NvClust 결과 최적의 군집수는 2개로 나왔지만 수가 너무 적으므로, 득표를 받은 군집 중에 가장 수가 많은 29개를 군집수로 하여 군집화 시킵니다.

```bash
library(dplyr)
for (i in 1:29){
  print(c1 %>% filter(`cl$clustering`==i))}
```

각 군집별 무슨 단어들이 모여있는지 확인합니다.

먼저 1점 리뷰의 군집입니다.

![https://blog.kakaocdn.net/dn/BR1m4/btrMy1y7xDB/GtEuHaVGkgYIjikbTQT7k1/img.png](https://blog.kakaocdn.net/dn/BR1m4/btrMy1y7xDB/GtEuHaVGkgYIjikbTQT7k1/img.png)

1점 리뷰의 단어 군집

29개의 군집 중 주요한 단어들이 있는 군집들을 가져왔습니다.

군집화 하여 함께 오는 단어를 확인하니 보다 명확하게 판단할 수 있습니다.

상품, 최악, 실망, 더럽다, 흘리다 등의 단어가 함께오고,

보다 짧은, 작은, 때문에, 화나다, 등의 단어가 함께 오고 있습니다.

![image (4)](https://github.com/user-attachments/assets/0e9e6df0-cc5d-45c8-a326-a65b48903d3e)

5점 리뷰의 단어군집

5점 리뷰의 경우 장문으로 되어있는 경우가 많아 보다 많은 단어들이 나타났지만, 전반적으로 다른 음식과 우유를 함께 먹었다는 자신의 경험을 얘기하는 듯한 느낌의 군집들이 많았습니다.

**모델링**

이번에는 리뷰 데이터로 별점을 예측하는 예측 모델을 만들어 보도록 하겠습니다.

```python
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D
from tensorflow.keras.layers import Embedding, Dropout, Flatten
from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional
from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D
from tensorflow.keras.utils import plot_model, to_categorical
from keras.utils import np_utils
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import OneHotEncoder

tf.random.set_seed(1234)
star_review=pd.read_excel('/content/star_review.xlsx')
for i in range(2881):   star_review['리뷰'][i] = re.sub('[^가-힣]', ' ', str(star_review['리뷰'][i]))#한글 이외 전부 제거

train, test = train_test_split(star_review, test_size=0.2)
train_text=train['리뷰']; train_label=train['별점']; test_text=test['리뷰']; test_label=test['별점']

tokenizer=Tokenizer()
tokenizer.fit_on_texts(train_text)
train_sequences=tokenizer.texts_to_sequences(train_text)
test_sequences=tokenizer.texts_to_sequences(test_text)

train_input=pad_sequences(train_sequences, maxlen=500, padding='post')
train_label=np_utils.to_categorical(train_label)
test_input=pad_sequences(test_sequences, maxlen=500, padding='post')
test_label=np_utils.to_categorical(test_label)

model = Sequential([Embedding(len(tokenizer.word_index)+1, 200, input_length=500),
        tf.keras.layers.Bidirectional(LSTM(units = 16, return_sequences = True)),
        tf.keras.layers.Bidirectional(LSTM(units = 16, return_sequences = True)),
        tf.keras.layers.Bidirectional(LSTM(units = 16)),
        Dense(6, activation='softmax')
    ])

 adam=Adam(learning_rate=4e-4)
model.compile(optimizer=adam, loss = 'categorical_crossentropy', metrics=['acc'])
history=model.fit(train_input, train_label, epochs=15, batch_size=128, verbose=1, validation_split=0.2)
score=model.evaluate(test_input, test_label)
print(score)
```

![https://blog.kakaocdn.net/dn/bJsgOX/btrNcZNsGlJ/R5kKLJB0elRa2vvjhfHy70/img.png](https://blog.kakaocdn.net/dn/bJsgOX/btrNcZNsGlJ/R5kKLJB0elRa2vvjhfHy70/img.png)

위 코드론 양방향 LSTM이 나타나 있습니다. rnn과 cnn 일방향 lstm등의 코드를 돌려 본 결과 양방향 lstm이 가장 좋은 결과를 얻었으며, val acc가 약 50% 수준으로 올라가지 않는 문제를 겪어 드롭아웃과 정칙화 규제를 추가하였으나 더 좋지 않은 결과를 얻었습니다.  클래스당 약 500개 정도의 매우 적은 데이터의 수로 모델링을 수행하였으며, 안그래도 적은 정보인데 학습특징을 퇴출시키거나 규제를 가하는 행위는 더 안좋은 결과를 불러 일으킨거라 판단됩니다. 그나마 가장 효과를 보았던것은 64개의 unit수를 16개로 줄여 모델의 기억 용량을 줄이는 식의 방식으로 0.51의 val acc를 0.57까지 나오게 만들었습니다.

딥러닝 모델이 항상 머신러닝 모델보다 좋은 결과를 얻는 것은 아니기에 랜덤 포레스트 모델로 한번 작업을 다시 수행해봤습니다.

```bash
import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

star_review=pd.read_excel('/content/star_review.xlsx')
#for i in range(2881):   star_review['영문리뷰'][i] = re.sub('[^a-zA-Z]', ' ', str(star_review['영문리뷰'][i])) #영어 이외 제거for i in range(2881):   star_review['리뷰'][i] = re.sub('[^가-힣]', ' ', str(star_review['리뷰'][i]))#한글 이외 전부 제거#for i in range(2881):   star_review['영문리뷰'][i]=star_review['영문리뷰'][i].lower() #소문자화

text=list(star_review['리뷰'])
label=np.array(star_review['별점'])

tfidf=TfidfVectorizer(min_df=0, analyzer='char', sublinear_tf=True, ngram_range=(1,3), max_features=5000)
text=tfidf.fit_transform(text)

train_text, test_text, train_label, test_label = train_test_split(text, label, test_size=0.3, random_state=1234)
from sklearn.ensemble import RandomForestClassifier
RF=RandomForestClassifier(n_estimators=100)#100개의 의사결정나무
RF.fit(train_text, train_label)#트레인의 텍스트와 레이블print("Accuracy: %f"% RF.score(test_text, test_label))#검증 데이터로 성능 측정
```

![https://blog.kakaocdn.net/dn/LgkGL/btrMTLput76/BtJPCcwUp5EUba9kKebEi0/img.png](https://blog.kakaocdn.net/dn/LgkGL/btrMTLput76/BtJPCcwUp5EUba9kKebEi0/img.png)

랜덤 포레스트 모델의 정확도는 60% 수준으로 딥러닝 모델 보다 더 나은 성능을 보였습니다.

크롤링으로 보다 많은 데이터를 수집한다면 더 나은 결과를 얻을 수 있겠지만 예측 모델링은 여기서 마치도록 하겠습니다.

이상으로 쿠팡 서울우유 리뷰 데이터 분석을 마칩니다.
